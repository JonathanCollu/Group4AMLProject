{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PTC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICTIVE TERMINATION CRITERION**\n",
        "\n",
        "This notebook aims to experiment the predictive termination criterion described in \"Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-fourth international joint conference on artificial intelligence. 2015.\" on several Skitlearn learners and datasets from OpenML\n"
      ],
      "metadata": {
        "id": "JKTwHCseekKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fullfill requirements**"
      ],
      "metadata": {
        "id": "LOtbHBo_gEdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evqa2Foaefav",
        "outputId": "1ad14bc0-ea89-4f46-a3a2-95f7a12dc3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emcee\n",
            "  Downloading emcee-3.1.1-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▏                        | 10 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 20 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 45 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from emcee) (1.19.5)\n",
            "Installing collected packages: emcee\n",
            "Successfully installed emcee-3.1.1\n",
            "Collecting triangle\n",
            "  Downloading triangle-20200424-cp37-cp37m-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from triangle) (1.19.5)\n",
            "Installing collected packages: triangle\n",
            "Successfully installed triangle-20200424\n",
            "Collecting openml\n",
            "  Downloading openml-0.12.2.tar.gz (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting liac-arff>=2.4.0\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openml) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from openml) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openml) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from openml) (1.1.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from openml) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from openml) (1.19.5)\n",
            "Collecting minio\n",
            "  Downloading minio-7.1.2-py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from openml) (3.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openml) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->openml) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->openml) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->openml) (3.0.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from minio->openml) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from minio->openml) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openml) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openml) (3.0.4)\n",
            "Building wheels for collected packages: openml, liac-arff\n",
            "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.12.2-py3-none-any.whl size=137327 sha256=7743aba6070807b1a801c454e743142c23eef8cd4f38d1c4d008e2dc9d92849f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/20/88/cf4ac86aa18e2cd647ed16ebe274a5dacee9d0075fa02af250\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11731 sha256=2e9a6451f1350687ed90abb980fe6742e5c6891bfebf3e8c0dee8149c9f4bdd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
            "Successfully built openml liac-arff\n",
            "Installing collected packages: xmltodict, minio, liac-arff, openml\n",
            "Successfully installed liac-arff-2.5.0 minio-7.1.2 openml-0.12.2 xmltodict-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emcee\n",
        "!pip install triangle\n",
        "!pip install openml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import section**"
      ],
      "metadata": {
        "id": "wm1G6P2Ugele"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import emcee\n",
        "import openml\n",
        "import random\n",
        "import logging\n",
        "import inspect\n",
        "import triangle\n",
        "import traceback\n",
        "import numpy as np\n",
        "from pylab import *\n",
        "from functools import reduce\n",
        "from scipy.stats import norm\n",
        "from functools import partial\n",
        "from sklearn.base import clone\n",
        "from scipy.stats import norm, kde\n",
        "from sklearn.svm import LinearSVC\n",
        "from scipy.special import logsumexp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from scipy.optimize import curve_fit, leastsq, fmin_bfgs, fmin_l_bfgs_b, nnls\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier, SGDClassifier"
      ],
      "metadata": {
        "id": "vKvpQPFkgjjE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curve Functions**"
      ],
      "metadata": {
        "id": "xrcX_4GBjmQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#all the models that we considered at some point\n",
        "all_models = {}\n",
        "model_defaults = {}\n",
        "display_name_mapping = {}\n",
        "\n",
        "\n",
        "def pow3(x, c, a, alpha):\n",
        "    return c - a * x**(-alpha)\n",
        "all_models[\"pow3\"] = pow3\n",
        "model_defaults[\"pow3\"] = {\"c\": 0.84, \"a\": 0.52, \"alpha\": 0.01}\n",
        "display_name_mapping[\"pow3\"] = \"pow$_3$\"\n",
        "\n",
        "def linear(x, a, b):\n",
        "    return a*x + b\n",
        "#models[\"linear\"] = linear\n",
        "all_models[\"linear\"] = linear\n",
        "\n",
        "\"\"\"\n",
        "    Source: curve expert\n",
        "\"\"\"\n",
        "def log_power(x, a, b, c):\n",
        "    #logistic power\n",
        "    return a/(1.+(x/np.exp(b))**c)\n",
        "all_models[\"log_power\"] = log_power\n",
        "model_defaults[\"log_power\"] = {\"a\": 0.77, \"c\": -0.51, \"b\": 2.98}\n",
        "display_name_mapping[\"log_power\"] = \"log power\"\n",
        "\n",
        "def weibull(x, alpha, beta, kappa, delta):\n",
        "    \"\"\"\n",
        "    Weibull modell\n",
        "\n",
        "    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n",
        "\n",
        "    alpha: upper asymptote\n",
        "    beta: lower asymptote\n",
        "    k: growth rate\n",
        "    delta: controls the x-ordinate for the point of inflection\n",
        "    \"\"\"\n",
        "    return alpha - (alpha - beta) * np.exp(-(kappa * x)**delta)\n",
        "all_models[\"weibull\"] = weibull\n",
        "model_defaults[\"weibull\"] = {\"alpha\": .7, \"beta\": 0.1, \"kappa\": 0.01,  \"delta\": 1}\n",
        "display_name_mapping[\"weibull\"] = \"Weibull\"\n",
        "\n",
        "\n",
        "def mmf(x, alpha, beta, kappa, delta):\n",
        "    \"\"\"\n",
        "        Morgan-Mercer-Flodin\n",
        "\n",
        "        description:\n",
        "        Nonlinear Regression page 342\n",
        "        http://bit.ly/1jodG17\n",
        "        http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm  \n",
        "\n",
        "        alpha: upper asymptote\n",
        "        kappa: growth rate\n",
        "        beta: initial value\n",
        "        delta: controls the point of inflection\n",
        "    \"\"\"\n",
        "    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n",
        "all_models[\"mmf\"] = mmf\n",
        "model_defaults[\"mmf\"] = {\"alpha\": .7, \"kappa\": 0.01, \"beta\": 0.1, \"delta\": 5}\n",
        "display_name_mapping[\"mmf\"] = \"MMF\"\n",
        "\n",
        "def janoschek(x, a, beta, k, delta):\n",
        "    \"\"\"\n",
        "        http://www.pisces-conservation.com/growthhelp/janoschek.htm\n",
        "    \"\"\"\n",
        "    return a - (a - beta) * np.exp(-k*x**delta)\n",
        "all_models[\"janoschek\"] = janoschek\n",
        "model_defaults[\"janoschek\"] = {\"a\": 0.73, \"beta\": 0.07, \"k\": 0.355, \"delta\": 0.46}\n",
        "display_name_mapping[\"janoschek\"] = \"Janoschek\"\n",
        "\n",
        "def ilog2(x, c, a):\n",
        "    x = 1 + x\n",
        "    assert(np.all(x>1))\n",
        "    return c - a / np.log(x)\n",
        "all_models[\"ilog2\"] = ilog2\n",
        "model_defaults[\"ilog2\"] = {\"a\": 0.43, \"c\": 0.78}\n",
        "display_name_mapping[\"ilog2\"] = \"ilog$_2$\"\n",
        "\n",
        "\n",
        "def dr_hill_zero_background(x, theta, eta, kappa):\n",
        "    return (theta* x**eta) / (kappa**eta + x**eta)\n",
        "all_models[\"dr_hill_zero_background\"] = dr_hill_zero_background\n",
        "model_defaults[\"dr_hill_zero_background\"] = {\"theta\": 0.772320, \"eta\": 0.586449, \"kappa\": 2.460843}\n",
        "display_name_mapping[\"dr_hill_zero_background\"] = \"Hill$_3$\"\n",
        "\n",
        "\n",
        "def logx_linear(x, a, b):\n",
        "    x = np.log(x)\n",
        "    return a*x + b\n",
        "all_models[\"logx_linear\"] = logx_linear\n",
        "model_defaults[\"logx_linear\"] = {\"a\": 0.378106, \"b\": 0.046506}\n",
        "display_name_mapping[\"logx_linear\"] = \"log x linear\"\n",
        "\n",
        "\n",
        "def vap(x, a, b, c):\n",
        "    \"\"\" Vapor pressure model \"\"\"\n",
        "    return np.exp(a+b/x+c*np.log(x))\n",
        "all_models[\"vap\"] = vap\n",
        "model_defaults[\"vap\"] = {\"a\": -0.622028, \"c\": 0.042322, \"b\": -0.470050}\n",
        "display_name_mapping[\"vap\"] = \"vapor pressure\"\n",
        "\n",
        "\n",
        "\n",
        "def loglog_linear(x, a, b):\n",
        "    x = np.log(x)\n",
        "    return np.log(a*x + b)\n",
        "all_models[\"loglog_linear\"] = loglog_linear\n",
        "display_name_mapping[\"loglog_linear\"] = \"log log linear\"\n",
        "\n",
        "\n",
        "#Models that we chose not to use in the ensembles/model combinations:\n",
        "\n",
        "#source: http://aclweb.org/anthology//P/P12/P12-1003.pdf\n",
        "def exp3(x, c, a, b):\n",
        "    return c - np.exp(-a*x+b)\n",
        "all_models[\"exp3\"] = exp3\n",
        "model_defaults[\"exp3\"] = {\"c\": 0.7, \"a\": 0.01, \"b\": -1}\n",
        "display_name_mapping[\"exp3\"] = \"exp$_3$\"\n",
        "\n",
        "\n",
        "def exp4(x, c, a, b, alpha):\n",
        "    return c - np.exp(-a*(x**alpha)+b)\n",
        "all_models[\"exp4\"] = exp4\n",
        "model_defaults[\"exp4\"] = {\"c\": 0.7, \"a\":  0.8, \"b\":-0.8, \"alpha\": 0.3}\n",
        "display_name_mapping[\"exp4\"] = \"exp$_4$\"\n",
        "\n",
        "def pow2(x, a, alpha):\n",
        "    return a * x**(-alpha)\n",
        "all_models[\"pow2\"] = pow2\n",
        "model_defaults[\"pow2\"] = {\"a\": 0.1, \"alpha\": -0.3}\n",
        "display_name_mapping[\"pow2\"] = \"pow$_2$\"\n",
        "\n",
        "def pow4(x, c, a, b, alpha):\n",
        "    return c - (a*x+b)**-alpha\n",
        "all_models[\"pow4\"] = pow4\n",
        "model_defaults[\"pow4\"] = {\"alpha\": 0.1, \"a\": 200, \"b\": 0., \"c\": 0.8}\n",
        "display_name_mapping[\"pow4\"] = \"pow$_4$\"\n",
        "\n",
        "\n",
        "\n",
        "def sat_growth(x, a, b):\n",
        "    return a * x / (b + x)\n",
        "all_models[\"sat_growth\"] = sat_growth\n",
        "model_defaults[\"sat_growth\"] = {\"a\": 0.7, \"b\": 20}\n",
        "display_name_mapping[\"sat_growth\"] = \"saturated growth rate\"\n",
        "\n",
        "\n",
        "def dr_hill(x, alpha, theta, eta, kappa):\n",
        "    return alpha + (theta*(x**eta)) / (kappa**eta + x**eta)\n",
        "all_models[\"dr_hill\"] = dr_hill\n",
        "model_defaults[\"dr_hill\"] = {\"alpha\": 0.1, \"theta\": 0.772320, \"eta\": 0.586449, \"kappa\": 2.460843}\n",
        "display_name_mapping[\"dr_hill\"] = \"Hill$_4$\"\n",
        "\n",
        "\n",
        "\n",
        "def gompertz(x, a, b, c):\n",
        "    \"\"\"\n",
        "        Gompertz growth function.\n",
        "        \n",
        "        sigmoidal family\n",
        "        a is the upper asymptote, since\n",
        "        b, c are negative numbers\n",
        "        b sets the displacement along the x axis (translates the graph to the left or right)\n",
        "        c sets the growth rate (y scaling)\n",
        "        \n",
        "        e.g. used to model the growth of tumors\n",
        "        \n",
        "        http://en.wikipedia.org/wiki/Gompertz_function\n",
        "    \"\"\"\n",
        "    return a*np.exp(-b*np.exp(-c*x))\n",
        "    #return a + b * np.exp(np.exp(-k*(x-i)))\n",
        "all_models[\"gompertz\"] = gompertz\n",
        "model_defaults[\"gompertz\"] = {\"a\": 0.8, \"b\": 1000, \"c\": 0.05}\n",
        "display_name_mapping[\"gompertz\"] = \"Gompertz\"\n",
        "\n",
        "\n",
        "def logistic_curve(x, a, k, b):\n",
        "    \"\"\"\n",
        "        a: asymptote\n",
        "        k: \n",
        "        b: inflection point\n",
        "        http://www.pisces-conservation.com/growthhelp/logistic_curve.htm\n",
        "    \"\"\"\n",
        "    return a / (1. + np.exp(-k*(x-b)))\n",
        "all_models[\"logistic_curve\"] = logistic_curve\n",
        "model_defaults[\"logistic_curve\"] = {\"a\": 0.8, \"k\": 0.01, \"b\": 1.}\n",
        "display_name_mapping[\"logistic_curve\"] = \"logistic curve\"\n",
        "\n",
        "\n",
        "\n",
        "def bertalanffy(x, a, k):\n",
        "    \"\"\"\n",
        "        a: asymptote\n",
        "        k: growth rate\n",
        "        http://www.pisces-conservation.com/growthhelp/von_bertalanffy.htm\n",
        "    \"\"\"\n",
        "    return a * (1. - np.exp(-k*x))\n",
        "all_models[\"bertalanffy\"] = bertalanffy\n",
        "model_defaults[\"bertalanffy\"] = {\"a\": 0.8, \"k\": 0.01}\n",
        "display_name_mapping[\"bertalanffy\"] = \"Bertalanffy\"\n",
        "\n",
        "\n",
        "\n",
        "curve_combination_models_old = [\"vap\", \"ilog2\", \"weibull\", \"pow3\", \"pow4\", \"loglog_linear\",\n",
        "                                \"mmf\", \"janoschek\", \"dr_hill_zero_background\", \"log_power\",\n",
        "                                \"exp4\"]\n",
        "\n",
        "\n",
        "curve_combination_models = [\"weibull\", \"pow4\", \"mmf\", \"pow3\", \"loglog_linear\",\n",
        "                            \"janoschek\", \"dr_hill_zero_background\", \"log_power\", \"exp4\"]\n",
        "\n",
        "curve_ensemble_models = [\"vap\", \"ilog2\", \"weibull\", \"pow3\", \"pow4\", \"loglog_linear\",\n",
        "                         \"mmf\", \"janoschek\", \"dr_hill_zero_background\", \"log_power\",\n",
        "                         \"exp4\"]"
      ],
      "metadata": {
        "id": "0bAVznc3kCxY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curve Models**"
      ],
      "metadata": {
        "id": "KmL-wPzskPvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recency_weights(num):\n",
        "    if num == 1:\n",
        "        return np.ones(1)\n",
        "    else:\n",
        "        recency_weights = [10**(1./num)] * num\n",
        "        recency_weights = recency_weights**(np.arange(0, num))\n",
        "        return recency_weights\n",
        "\n",
        "\n",
        "def masked_mean_x_greater_than(posterior_distribution, y):\n",
        "    \"\"\"\n",
        "        P(E[f(x)] > E[y] | Data)\n",
        "    \"\"\"\n",
        "    predictions = np.ma.masked_invalid(posterior_distribution)\n",
        "    return np.sum(predictions > y) / float(np.sum(~predictions.mask))\n",
        "\n",
        "\n",
        "class CurveModel(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 function,\n",
        "                 function_der=None,\n",
        "                 min_vals={},\n",
        "                 max_vals={},\n",
        "                 default_vals={}):\n",
        "        \"\"\"\n",
        "            function: the function to be fit\n",
        "            function_der: derivative of that function\n",
        "        \"\"\"\n",
        "        self.function = function\n",
        "        if function_der != None:\n",
        "            raise NotImplementedError(\"function derivate is not implemented yet...sorry!\")\n",
        "        self.function_der = function_der\n",
        "        assert isinstance(min_vals, dict)\n",
        "        self.min_vals = min_vals.copy()\n",
        "        assert isinstance(max_vals, dict)\n",
        "        self.max_vals = max_vals.copy()\n",
        "        function_args = inspect.getargspec(function).args\n",
        "        assert \"x\" in function_args, \"The function needs 'x' as a parameter.\"\n",
        "        for default_param_name in default_vals.keys():\n",
        "            if default_param_name == \"sigma\":\n",
        "                continue\n",
        "            msg = \"function %s doesn't take default param %s\" % (function.__name__, default_param_name)\n",
        "            assert default_param_name in function_args, msg\n",
        "        self.function_params = [param for param in function_args if param != 'x']\n",
        "        #set default values:\n",
        "        self.default_vals = default_vals.copy()\n",
        "        for param_name in self.function_params:\n",
        "            if param_name not in default_vals:\n",
        "                print(\"setting function parameter %s to default of 1.0 for function %s\" % (param_name,\n",
        "                                                                                           self.function.__name__))\n",
        "                self.default_vals[param_name] = 1.0\n",
        "        self.all_param_names = [param for param in self.function_params]\n",
        "        self.all_param_names.append(\"sigma\")\n",
        "        self.name = self.function.__name__\n",
        "        self.ndim = len(self.all_param_names)\n",
        "        \n",
        "        #uniform noise prior over interval:\n",
        "        if \"sigma\" not in self.min_vals:\n",
        "            self.min_vals[\"sigma\"] = 0.\n",
        "        if \"sigma\" not in self.max_vals:\n",
        "            self.max_vals[\"sigma\"] = 1.0\n",
        "        if \"sigma\" not in self.default_vals:\n",
        "            self.default_vals[\"sigma\"] = 0.05\n",
        "    \n",
        "    def default_function_param_array(self):\n",
        "        return np.asarray([self.default_vals[param_name] for param_name in self.function_params])\n",
        "\n",
        "    def are_params_in_bounds(self, theta):\n",
        "        \"\"\"\n",
        "            Are the parameters in their respective bounds?\n",
        "        \"\"\"\n",
        "        in_bounds = True\n",
        "        \n",
        "        for param_name, param_value in zip(self.all_param_names, theta):\n",
        "            if param_name in self.min_vals:\n",
        "                if param_value < self.min_vals[param_name]:\n",
        "                    in_bounds = False\n",
        "            if param_name in self.max_vals:\n",
        "                if param_value > self.max_vals[param_name]:\n",
        "                    in_bounds = False\n",
        "        return in_bounds\n",
        "\n",
        "    def split_theta(self, theta):\n",
        "        \"\"\"Split theta into the function parameters (dict) and sigma. \"\"\"\n",
        "        params = {}\n",
        "        sigma = None\n",
        "        for param_name, param_value in zip(self.all_param_names, theta):\n",
        "            if param_name in self.function_params:\n",
        "                params[param_name] = param_value\n",
        "            elif param_name == \"sigma\":\n",
        "                sigma = param_value\n",
        "        return params, sigma\n",
        "\n",
        "    def split_theta_to_array(self, theta):\n",
        "        \"\"\"Split theta into the function parameters (array) and sigma. \"\"\"\n",
        "        params = theta[:-1]\n",
        "        sigma = theta[-1]\n",
        "        return params, sigma\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self, x):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict_given_theta(self, x, theta):\n",
        "        \"\"\"\n",
        "            Make predictions given a single theta\n",
        "        \"\"\"\n",
        "        params, sigma = self.split_theta(theta)\n",
        "        predictive_mu = self.function(x, **params)\n",
        "        return predictive_mu, sigma\n",
        "\n",
        "    def likelihood(self, x, y):\n",
        "        \"\"\"\n",
        "            for each y_i in y:\n",
        "                p(y_i|x, model)\n",
        "        \"\"\"\n",
        "        params, sigma = self.split_theta(self.ml_params)\n",
        "        return norm.pdf(y-self.function(x, **params), loc=0, scale=sigma)\n",
        "\n",
        "\n",
        "class MLCurveModel(CurveModel):\n",
        "    \"\"\"\n",
        "        ML fit of a curve.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, recency_weighting=True,  **kwargs):\n",
        "        super(MLCurveModel, self).__init__(**kwargs)\n",
        "\n",
        "        #Maximum Likelihood values of the parameters\n",
        "        self.ml_params = None\n",
        "        self.recency_weighting = recency_weighting\n",
        "\n",
        "    def fit(self, x, y, weights=None, start_from_default=True):\n",
        "        \"\"\"\n",
        "            weights: None or weight for each sample.\n",
        "        \"\"\"\n",
        "        return self.fit_ml(x, y, weights, start_from_default)\n",
        "\n",
        "    def predict(self, x):\n",
        "        #assert len(x.shape) == 1\n",
        "        params, sigma = self.split_theta_to_array(self.ml_params)\n",
        "        return self.function(x, *params)\n",
        "        #return np.asarray([self.function(x_pred, **params) for x_pred in x])\n",
        "\n",
        "    def fit_ml(self, x, y, weights, start_from_default):\n",
        "        \"\"\"\n",
        "            non-linear least-squares fit of the data.\n",
        "\n",
        "            First tries Levenberg-Marquardt and falls back\n",
        "            to BFGS in case that fails.\n",
        "\n",
        "            Start from default values or from previous ml_params?\n",
        "        \"\"\"\n",
        "        successful = self.fit_leastsq(x, y, weights, start_from_default)\n",
        "        if not successful:\n",
        "            successful = self.fit_bfgs(x, y, weights, start_from_default)\n",
        "            if not successful:\n",
        "                return False\n",
        "        return successful\n",
        "\n",
        "    def ml_sigma(self, x, y, popt, weights):\n",
        "        \"\"\"\n",
        "            Given the ML parameters (popt) get the ML estimate of sigma.\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            if self.recency_weighting:\n",
        "                variance = np.average((y-self.function(x, *popt))**2,\n",
        "                    weights=recency_weights(len(y)))\n",
        "                sigma = np.sqrt(variance)\n",
        "            else:\n",
        "                sigma = (y-self.function(x, *popt)).std()\n",
        "        else:\n",
        "            if self.recency_weighting:\n",
        "                variance = np.average((y-self.function(x, *popt))**2,\n",
        "                    weights=recency_weights(len(y)) * weights)\n",
        "                sigma = np.sqrt(variance)\n",
        "            else:\n",
        "                variance = np.average((y-self.function(x, *popt))**2,\n",
        "                    weights=weights)\n",
        "                sigma = np.sqrt(variance)\n",
        "        return sigma\n",
        "\n",
        "    def fit_leastsq(self, x, y, weights, start_from_default):    \n",
        "        try:\n",
        "            if weights is None:\n",
        "                if self.recency_weighting:\n",
        "                    residuals = lambda p: np.sqrt(recency_weights(len(y))) * (self.function(x, *p) - y)\n",
        "                else:\n",
        "                    residuals = lambda p: self.function(x, *p) - y\n",
        "            else:\n",
        "                #the return value of this function will be squared, hence\n",
        "                #we need to take the sqrt of the weights here\n",
        "                if self.recency_weighting:\n",
        "                    residuals = lambda p: np.sqrt(recency_weights(len(y))*weights) * (self.function(x, *p) - y)\n",
        "                else:\n",
        "                    residuals = lambda p: np.sqrt(weights) * (self.function(x, *p) - y)\n",
        "\n",
        "            \n",
        "            if start_from_default:\n",
        "                initial_params = self.default_function_param_array()\n",
        "            else:\n",
        "                initial_params, _ = self.split_theta_to_array(self.ml_params)\n",
        "\n",
        "            popt, cov_popt, info, msg, status = leastsq(residuals,\n",
        "                    x0=initial_params,\n",
        "                    full_output=True)\n",
        "                #Dfun=,\n",
        "                #col_deriv=True)\n",
        "            \n",
        "            if np.any(np.isnan(info[\"fjac\"])):\n",
        "                return False\n",
        "\n",
        "            leastsq_success_statuses = [1,2,3,4]\n",
        "            if status in leastsq_success_statuses:\n",
        "                if any(np.isnan(popt)):\n",
        "                    return False\n",
        "                #within bounds?\n",
        "                if not self.are_params_in_bounds(popt):\n",
        "                    return False\n",
        "\n",
        "                sigma = self.ml_sigma(x, y, popt, weights)\n",
        "                self.ml_params = np.append(popt, [sigma])\n",
        "\n",
        "                logging.info(\"leastsq successful for model %s\" % self.function.__name__)\n",
        "\n",
        "                return True\n",
        "            else:\n",
        "                logging.warn(\"leastsq NOT successful for model %s, msg: %s\" % (self.function.__name__, msg))\n",
        "                logging.warn(\"best parameters found: \" + str(popt))\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            tb = traceback.format_exc()\n",
        "            print(tb)\n",
        "            return False\n",
        "\n",
        "    def fit_bfgs(self, x, y, weights, start_from_default):\n",
        "        try:\n",
        "            def objective(params):\n",
        "                if weights is None:\n",
        "                    if self.recency_weighting:\n",
        "                        return np.sum(recency_weights(len(y))*(self.function(x, *params) - y)**2)\n",
        "                    else:\n",
        "                        return np.sum((self.function(x, *params) - y)**2)\n",
        "                else:\n",
        "                    if self.recency_weighting:\n",
        "                        return np.sum(weights * recency_weights(len(y)) * (self.function(x, *params) - y)**2)\n",
        "                    else:\n",
        "                        return np.sum(weights * (self.function(x, *params) - y)**2)\n",
        "            bounds = []\n",
        "            for param_name in self.function_params:\n",
        "                if param_name in self.min_vals and param_name in self.max_vals:\n",
        "                    bounds.append((self.min_vals[param_name], self.max_vals[param_name]))\n",
        "                elif param_name in self.min_vals:\n",
        "                    bounds.append((self.min_vals[param_name], None))\n",
        "                elif param_name in self.max_vals:\n",
        "                    bounds.append((None, self.max_vals[param_name]))\n",
        "                else:\n",
        "                    bounds.append((None, None))\n",
        "\n",
        "            if start_from_default:\n",
        "                initial_params = self.default_function_param_array()\n",
        "            else:\n",
        "                initial_params, _ = self.split_theta_to_array(self.ml_params)\n",
        "\n",
        "            popt, fval, info= fmin_l_bfgs_b(objective,\n",
        "                                            x0=initial_params,\n",
        "                                            bounds=bounds,\n",
        "                                            approx_grad=True)\n",
        "            if info[\"warnflag\"] != 0:\n",
        "                logging.warn(\"BFGS not converged! (warnflag %d) for model %s\" % (info[\"warnflag\"], self.name))\n",
        "                logging.warn(info)\n",
        "                return False\n",
        "\n",
        "            if popt is None:\n",
        "                return False\n",
        "            if any(np.isnan(popt)):\n",
        "                logging.info(\"bfgs NOT successful for model %s, parameter NaN\" % self.name)\n",
        "                return False\n",
        "            sigma = self.ml_sigma(x, y, popt, weights)\n",
        "            self.ml_params = np.append(popt, [sigma])\n",
        "            logging.info(\"bfgs successful for model %s\" % self.name)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def aic(self, x, y):\n",
        "        \"\"\"\n",
        "            Akaike information criterion\n",
        "            http://en.wikipedia.org/wiki/Akaike_information_criterion\n",
        "        \"\"\"\n",
        "        params, sigma = self.split_theta_to_array(self.ml_params)\n",
        "        y_model = self.function(x, *params)\n",
        "        log_likelihood = norm.logpdf(y-y_model, loc=0, scale=sigma).sum()\n",
        "        return 2 * len(self.function_params) - 2 * log_likelihood\n",
        "\n",
        "\n",
        "\n",
        "class MCMCCurveModel(CurveModel):\n",
        "    \"\"\"\n",
        "        MLE curve fitting + MCMC sampling with uniform priors for parameter uncertainty.\n",
        "\n",
        "        Model: y ~ f(x) + eps with eps ~ N(0, sigma^2)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 function,\n",
        "                 function_der=None,\n",
        "                 min_vals={},\n",
        "                 max_vals={},\n",
        "                 default_vals={},\n",
        "                 burn_in=300,\n",
        "                 nwalkers=100,\n",
        "                 nsamples=800,\n",
        "                 nthreads=1,\n",
        "                 recency_weighting=False):\n",
        "        \"\"\"\n",
        "            function: the function to be fit\n",
        "            function_der: derivative of that function\n",
        "        \"\"\"\n",
        "        super(MCMCCurveModel, self).__init__(\n",
        "            function=function,\n",
        "            function_der=function_der,\n",
        "            min_vals=min_vals,\n",
        "            max_vals=max_vals,\n",
        "            default_vals=default_vals)\n",
        "        self.ml_curve_model = MLCurveModel(\n",
        "            function=function,\n",
        "            function_der=function_der,\n",
        "            min_vals=self.min_vals,\n",
        "            max_vals=self.max_vals,\n",
        "            default_vals=self.default_vals,\n",
        "            recency_weighting=recency_weighting)\n",
        "\n",
        "        #TODO: have two burn-ins, one for when the ML fitting is successful and one for when not!\n",
        "        self.burn_in = burn_in\n",
        "        self.nwalkers = nwalkers\n",
        "        self.nsamples = nsamples\n",
        "        self.nthreads = nthreads\n",
        "        self.recency_weighting = recency_weighting\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        try:\n",
        "            if self.ml_curve_model.fit(x, y):\n",
        "                logging.info(\"ML fit: \" + str(self.ml_curve_model.ml_params))\n",
        "                self.fit_mcmc(x, y)\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            tb = traceback.format_exc()\n",
        "            print(tb)\n",
        "            return False\n",
        "\n",
        "    #priors\n",
        "    def ln_prior(self, theta):\n",
        "        \"\"\"\n",
        "            log-prior is (up to a constant)\n",
        "        \"\"\"\n",
        "        if self.are_params_in_bounds(theta):\n",
        "            return 0.0\n",
        "        else:\n",
        "            return -np.inf\n",
        "    \n",
        "    #likelihood\n",
        "    def ln_likelihood(self, theta, x, y):\n",
        "        \"\"\"\n",
        "           y = y_true + y_noise\n",
        "            with y_noise ~ N(0, sigma^2)\n",
        "        \"\"\"\n",
        "        params, sigma = self.split_theta(theta)\n",
        "        y_model = self.function(x, **params)\n",
        "        if self.recency_weighting:\n",
        "            weight = recency_weights(len(y))\n",
        "            ln_likelihood = (weight*norm.logpdf(y-y_model, loc=0, scale=sigma)).sum()\n",
        "        else:\n",
        "            ln_likelihood = norm.logpdf(y-y_model, loc=0, scale=sigma).sum()\n",
        "        if np.isnan(ln_likelihood):\n",
        "            return -np.inf\n",
        "        else:\n",
        "            return ln_likelihood\n",
        "        \n",
        "    def ln_prob(self, theta, x, y):\n",
        "        \"\"\"\n",
        "            posterior probability\n",
        "        \"\"\"\n",
        "        lp = self.ln_prior(theta)\n",
        "        if not np.isfinite(lp):\n",
        "            return -np.inf\n",
        "        return lp + self.ln_likelihood(theta, x, y)\n",
        "    \n",
        "    def fit_mcmc(self, x, y):\n",
        "        #initialize in an area around the starting position\n",
        "        #pos = [start + 1e-4*np.random.randn(self.ndim) for i in range(self.nwalkers)]\n",
        "        assert self.ml_curve_model.ml_params is not None\n",
        "        pos = [self.ml_curve_model.ml_params + 1e-6*np.random.randn(self.ndim) for i in range(self.nwalkers)]\n",
        "        if self.nthreads <= 1:\n",
        "            sampler = emcee.EnsembleSampler(self.nwalkers,\n",
        "                self.ndim,\n",
        "                self.ln_prob,\n",
        "                args=(x, y))\n",
        "        else:\n",
        "            sampler = emcee.EnsembleSampler(\n",
        "                self.nwalkers,\n",
        "                self.ndim,\n",
        "                model_ln_prob,\n",
        "                args=(self, x, y),\n",
        "                threads=self.nthreads)\n",
        "        sampler.run_mcmc(pos, self.nsamples)\n",
        "        self.mcmc_chain = sampler.chain\n",
        "        \n",
        "    def get_burned_in_samples(self):\n",
        "        samples = self.mcmc_chain[:, self.burn_in:, :].reshape((-1, self.ndim))\n",
        "        return samples\n",
        "\n",
        "    def predictive_distribution(self, x, thin=1):\n",
        "        assert isinstance(x, float) or isinstance(x, int)\n",
        "        samples = self.get_burned_in_samples()\n",
        "        predictions = []\n",
        "        for theta in samples[::thin]:\n",
        "            params, sigma = self.split_theta(theta)\n",
        "            predictions.append(self.function(x, **params))\n",
        "        return np.asarray(predictions)\n",
        "\n",
        "    def predictive_ln_prob_distribution(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            posterior log p(y|x,D) for each sample\n",
        "        \"\"\"\n",
        "        #assert isinstance(x, float) or isinstance(x, int)\n",
        "        samples = self.get_burned_in_samples()\n",
        "        ln_probs = []\n",
        "        for theta in samples[::thin]:\n",
        "            ln_prob = self.ln_likelihood(theta, x, y)\n",
        "            ln_probs.append(ln_prob)\n",
        "        return np.asarray(ln_probs)\n",
        "\n",
        "    def posterior_ln_prob(self, x, y, thin=10):\n",
        "        \"\"\"\n",
        "            posterior log p(y|x,D)\n",
        "\n",
        "            1/S sum p(y|D,theta_s)\n",
        "            equivalent to:\n",
        "            logsumexp(log p(y|D,theta_s)) - log(S)\n",
        "        \"\"\"\n",
        "        assert not np.isscalar(x)\n",
        "        assert not np.isscalar(y)\n",
        "        x = np.asarray(x)\n",
        "        y = np.asarray(y)\n",
        "        ln_probs = self.predictive_ln_prob_distribution(x, y)\n",
        "        #print ln_probs\n",
        "        #print np.max(ln_probs)\n",
        "        #print np.min(ln_probs)\n",
        "        #print np.mean(ln_probs)\n",
        "        #print \"logsumexp(ln_probs)\", logsumexp(ln_probs)\n",
        "        #print \"np.log(len(ln_probs)) \", np.log(len(ln_probs))\n",
        "        #print logsumexp(ln_probs) - np.log(len(ln_probs))\n",
        "        return logsumexp(ln_probs) - np.log(len(ln_probs))\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "            E[f(x)]\n",
        "        \"\"\"\n",
        "        predictions = self.predictive_distribution(x)\n",
        "        return np.ma.masked_invalid(predictions).mean()\n",
        "    \n",
        "    def predictive_density(self, x_pos, x_density):\n",
        "        density = kde.gaussian_kde(self.predictive_distribution(x_pos))\n",
        "        return density(x_density)\n",
        "\n",
        "    def prob_x_greater_than(self, x, y, theta):\n",
        "        \"\"\"\n",
        "            P(f(x) > y | Data, theta)\n",
        "        \"\"\"\n",
        "        params, sigma = self.split_theta(theta)\n",
        "        mu = self.function(x, **params)\n",
        "        cdf = norm.cdf(y, loc=mu, scale=sigma)\n",
        "        return 1. - cdf\n",
        "\n",
        "    def posterior_mean_prob_x_greater_than(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            P(E[f(x)] > E[y] | Data)\n",
        "\n",
        "            thin: only use every thin'th sample\n",
        "        \n",
        "            Posterior probability that the expected valuef(x) is greater than \n",
        "            the expected value of y.\n",
        "        \"\"\"\n",
        "        posterior_distribution = self.predictive_distribution(x, thin)\n",
        "        return masked_mean_x_greater_than(posterior_distribution, y)\n",
        "\n",
        "\n",
        "    def posterior_prob_x_greater_than(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            P(f(x) > y | Data)\n",
        "        \n",
        "            Posterior probability that f(x) is greater than y.\n",
        "        \"\"\"\n",
        "        assert isinstance(x, float) or isinstance(x, int)\n",
        "        assert isinstance(y, float) or isinstance(y, int)\n",
        "        probs = []\n",
        "        samples = self.get_burned_in_samples()\n",
        "        for theta in samples[::thin]:\n",
        "            probs.append(self.prob_x_greater_than(x, y, theta))\n",
        "\n",
        "        return np.ma.masked_invalid(probs).mean()\n",
        "\n",
        "    def posterior_log_likelihoods(self, x, y):\n",
        "        #DEPRECATED!\n",
        "        samples = self.get_burned_in_samples()\n",
        "        log_likelihoods = []\n",
        "        for theta in samples:\n",
        "            params, sigma = self.split_theta(theta)\n",
        "            log_likelihood = self.ln_likelihood(theta, x, y)\n",
        "            #TODO: rather add a -np.inf?\n",
        "            if not np.isnan(log_likelihood) and np.isfinite(log_likelihood):\n",
        "                log_likelihoods.append(log_likelihood)\n",
        "        return log_likelihoods\n",
        "\n",
        "    def mean_posterior_log_likelihood(self, x, y):\n",
        "        #DEPRECATED!\n",
        "        return np.ma.masked_invalid(self.posterior_log_likelihoods(x, y)).mean()\n",
        "\n",
        "    def median_posterior_log_likelihood(self, x, y):\n",
        "        #DEPRECATED!\n",
        "        masked_x = np.ma.masked_invalid(self.posterior_log_likelihoods(x, y))\n",
        "        return np.ma.extras.median(masked_x)\n",
        "\n",
        "    def max_posterior_log_likelihood(self, x, y):\n",
        "        #DEPRECATED!\n",
        "        return np.ma.masked_invalid(self.posterior_log_likelihoods(x, y)).max()\n",
        "\n",
        "    def posterior_log_likelihood(self, x, y):\n",
        "        #DEPRECATED!\n",
        "        return self.median_posterior_log_likelihood(x, y)\n",
        "\n",
        "    def predictive_std(self, x, thin=1):\n",
        "        \"\"\"\n",
        "           sqrt(Var[f(x)])\n",
        "        \"\"\"\n",
        "        predictions = self.predictive_distribution(x, thin)\n",
        "        return np.ma.masked_invalid(predictions).std()\n",
        "\n",
        "    def dic(self, x, y):\n",
        "        \"\"\" Deviance Information Criterion. \"\"\"\n",
        "        samples = self.get_burned_in_samples()\n",
        "        deviances = []\n",
        "        for theta in samples:\n",
        "            params, sigma = self.split_theta(theta)\n",
        "            deviance = -2 * self.ln_likelihood(theta, x, y)\n",
        "            deviances.append(deviance)\n",
        "        mean_theta = samples.mean(axis=0)\n",
        "        theta_mean_deviance = -2 * self.ln_likelihood(mean_theta, x, y)\n",
        "        DIC = 2 * np.mean(deviances) - theta_mean_deviance\n",
        "        return DIC\n",
        "\n",
        "\n",
        "class LinearCurveModel(CurveModel):\n",
        "    \"\"\"\n",
        "        Fits a function f(x) = a * x + b using OLS.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *arg, **kwargs):\n",
        "        if \"default_vals\" in kwargs:\n",
        "            logging.warn(\"default values not needed for the linear model.\")\n",
        "        kwargs[\"default_vals\"] = {\"a\": 0, \"b\": 0}\n",
        "        kwargs[\"min_vals\"] = {\"a\": 0}\n",
        "        super(LinearCurveModel, self).__init__(\n",
        "                function=all_models[\"linear\"],\n",
        "                *arg,\n",
        "                **kwargs)\n",
        "\n",
        "    def fit(self, x, y, weights=None, start_from_default=True):\n",
        "        return self.fit_ml(x, y, weights)\n",
        "\n",
        "    def fit_ml(self, x, y, weights):\n",
        "        \"\"\"\n",
        "            Ordinary Least Squares fit.\n",
        "\n",
        "            TODO: use the weights!\n",
        "        \"\"\"\n",
        "        #TODO: check if the results agree with the minimum/maximum values!\n",
        "        X = np.asarray([np.ones(len(x)), x]).T\n",
        "        bh = np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,y))\n",
        "        a = bh[1]\n",
        "        b = bh[0]\n",
        "        sigma = (y-self.function(x, a, b)).std()\n",
        "        self.ml_params = np.asarray([a, b, sigma])\n",
        "        return True\n",
        "\n",
        "    def predict(self, x):\n",
        "        a = self.ml_params[0]\n",
        "        b = self.ml_params[1]\n",
        "        return a * x + b\n",
        "\n",
        "\n",
        "class LinearMCMCCurveModel(MCMCCurveModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        ml_curve_model = LinearCurveModel()\n",
        "        super(LinearMCMCCurveModel, self).__init__(\n",
        "            function=ml_curve_model.function,\n",
        "            min_vals=ml_curve_model.min_vals,\n",
        "            max_vals=ml_curve_model.max_vals,\n",
        "            default_vals=ml_curve_model.default_vals,\n",
        "            **kwargs)\n",
        "        self.ml_curve_model = ml_curve_model\n",
        "\n",
        "\n",
        "def model_ln_prob(theta, model, x, y):\n",
        "    return model.ln_prob(theta, x, y)\n",
        "\n",
        "\n",
        "class MCMCCurveModelCombination(object):\n",
        "\n",
        "    def __init__(self,\n",
        "            ml_curve_models,\n",
        "            xlim,\n",
        "            burn_in=500,\n",
        "            nwalkers=100,\n",
        "            nsamples=2500,\n",
        "            normalize_weights=True,\n",
        "            monotonicity_constraint=True,\n",
        "            soft_monotonicity_constraint=False,\n",
        "            initial_model_weight_ml_estimate=False,\n",
        "            normalized_weights_initialization=\"constant\",\n",
        "            strictly_positive_weights=True,\n",
        "            sanity_check_prior=True,\n",
        "            nthreads=1,\n",
        "            recency_weighting=True):\n",
        "        \"\"\"\n",
        "            xlim: the point on the x axis we eventually want to make predictions for.\n",
        "        \"\"\"\n",
        "        self.ml_curve_models = ml_curve_models\n",
        "        self.xlim = xlim\n",
        "        self.burn_in = burn_in\n",
        "        self.nwalkers = nwalkers\n",
        "        self.nsamples = nsamples\n",
        "        self.normalize_weights = normalize_weights\n",
        "        assert not (monotonicity_constraint and soft_monotonicity_constraint), \"choose either the monotonicity_constraint or the soft_monotonicity_constraint, but not both\"\n",
        "        self.monotonicity_constraint = monotonicity_constraint\n",
        "        self.soft_monotonicity_constraint = soft_monotonicity_constraint\n",
        "        self.initial_model_weight_ml_estimate = initial_model_weight_ml_estimate\n",
        "        self.normalized_weights_initialization = normalized_weights_initialization\n",
        "        self.strictly_positive_weights = strictly_positive_weights\n",
        "        self.sanity_check_prior = sanity_check_prior\n",
        "        self.nthreads = nthreads\n",
        "        self.recency_weighting = recency_weighting\n",
        "        #the constant used for initializing the parameters in a ball around the ML parameters\n",
        "        self.rand_init_ball = 1e-6\n",
        "        self.name = \"model combination\"# (%s)\" % \", \".join([model.name for model in self.ml_curve_models])\n",
        "\n",
        "    def fit(self, x, y, model_weights=None):\n",
        "        if self.fit_ml_individual(x, y, model_weights):\n",
        "            #run MCMC:\n",
        "            self.fit_mcmc(x, y)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"fit_ml_individual failed\")\n",
        "            return False\n",
        "\n",
        "    def y_lim_sanity_check(self, ylim):\n",
        "        # just make sure that the prediction is not below 0 nor insanely big\n",
        "        # HOWEVER: there might be cases where some models might predict value larger than 1.0\n",
        "        # and this is alright, because in those cases we don't necessarily want to stop a run.\n",
        "        if not np.isfinite(ylim) or ylim < 0. or ylim > 100.0:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def fit_ml_individual(self, x, y, model_weights):\n",
        "        \"\"\"\n",
        "            Do a ML fit for each model individually and then another ML fit for the combination of models.\n",
        "        \"\"\"\n",
        "        self.fit_models = []\n",
        "        for model in self.ml_curve_models:\n",
        "            if model.fit(x, y):\n",
        "                ylim = model.predict(self.xlim)\n",
        "                if not self.y_lim_sanity_check(ylim):\n",
        "                    print(\"ML fit of model %s is out of bound range [0.0, 100.] at xlim.\" % (model.function.__name__))\n",
        "                    continue\n",
        "                params, sigma = model.split_theta_to_array(model.ml_params)\n",
        "                if not np.isfinite(self.ln_model_prior(model, params)):\n",
        "                    print(\"ML fit of model %s is not supported by prior.\" % model.function.__name__)\n",
        "                    continue\n",
        "                self.fit_models.append(model)\n",
        "                    \n",
        "        if len(self.fit_models) == 0:\n",
        "            return False\n",
        "\n",
        "        if model_weights is None:\n",
        "            if self.normalize_weights:\n",
        "                if self.normalized_weights_initialization == \"constant\":\n",
        "                    #initialize with a constant value\n",
        "                    #we will sample in this unnormalized space and then later normalize\n",
        "                    model_weights = [10. for model in self.fit_models]\n",
        "                else:# self.normalized_weights_initialization == \"normalized\"\n",
        "                    model_weights = [1./len(self.fit_models) for model in self.fit_models]\n",
        "            else:\n",
        "                if self.initial_model_weight_ml_estimate:\n",
        "                    model_weights = self.get_ml_model_weights(x, y)\n",
        "                    print(model_weights)\n",
        "                    non_zero_fit_models = []\n",
        "                    non_zero_weights = []\n",
        "                    for w, model in zip(model_weights, self.fit_models):\n",
        "                        if w > 1e-4:\n",
        "                            non_zero_fit_models.append(model)\n",
        "                            non_zero_weights.append(w)\n",
        "                    self.fit_models = non_zero_fit_models\n",
        "                    model_weights = non_zero_weights\n",
        "                else:\n",
        "                    model_weights = [1./len(self.fit_models) for model in self.fit_models]\n",
        "\n",
        "        #build joint ml estimated parameter vector\n",
        "        model_params = []\n",
        "        all_model_params = []\n",
        "        for model in self.fit_models:\n",
        "            params, sigma = model.split_theta_to_array(model.ml_params)\n",
        "            model_params.append(params)\n",
        "            all_model_params.extend(params)\n",
        "\n",
        "        y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "        sigma = (y - y_predicted).std()\n",
        "\n",
        "        self.ml_params = self.join_theta(all_model_params, sigma, model_weights)\n",
        "        self.ndim = len(self.ml_params)\n",
        "        if self.nwalkers < 2*self.ndim:\n",
        "            self.nwalkers = 2*self.ndim\n",
        "            print(\"warning: increasing number of walkers to 2*ndim=%d\" % (self.nwalkers))\n",
        "        return True\n",
        "\n",
        "\n",
        "    def get_ml_model_weights(self, x, y_target):\n",
        "        \"\"\"\n",
        "            Get the ML estimate of the model weights.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "            Take all the models that have been fit using ML.\n",
        "            For each model we get a prediction of y: y_i\n",
        "\n",
        "            Now how can we combine those to reduce the squared error:\n",
        "\n",
        "                argmin_w (y_target - w_1 * y_1 - w_2 * y_2 - w_3 * y_3 ...)\n",
        "\n",
        "            Deriving and setting to zero we get a linear system of equations that we need to solve.\n",
        "\n",
        "\n",
        "            Resource on QP:\n",
        "            http://stats.stackexchange.com/questions/21565/how-do-i-fit-a-constrained-regression-in-r-so-that-coefficients-total-1\n",
        "            http://maggotroot.blogspot.de/2013/11/constrained-linear-least-squares-in.html\n",
        "        \"\"\"\n",
        "        num_models = len(self.fit_models)\n",
        "        y_predicted = []\n",
        "        b = []\n",
        "        for model in self.fit_models:\n",
        "            y_model = model.predict(x)\n",
        "            y_predicted.append(y_model)\n",
        "            b.append(y_model.dot(y_target))\n",
        "        a = np.zeros((num_models, num_models))\n",
        "        for i in range(num_models):\n",
        "            for j in range(num_models):\n",
        "                a[i, j] = y_predicted[i].dot(y_predicted[j])\n",
        "                #if i == j:\n",
        "                #    a[i, j] -= 0.1 #constraint the weights!\n",
        "        a_rank = np.linalg.matrix_rank(a)\n",
        "        if a_rank != num_models:\n",
        "            print(\"Rank %d not sufficcient for solving the linear system. %d needed at least.\" % (a_rank, num_models))\n",
        "        try:\n",
        "            print(np.linalg.lstsq(a, b)[0])\n",
        "            print(np.linalg.solve(a, b))\n",
        "            print(nnls(a, b)[0])\n",
        "            ##return np.linalg.solve(a, b)\n",
        "            weights = nnls(a, b)[0]\n",
        "            #weights = [w if w > 1e-4 else 1e-4 for w in weights]\n",
        "            return weights\n",
        "        #except LinAlgError as e:\n",
        "        except:\n",
        "            return [1./len(self.fit_models) for model in self.fit_models]\n",
        "\n",
        "\n",
        "    #priors\n",
        "    def ln_prior(self, theta):\n",
        "        ln = 0\n",
        "        model_params, sigma, model_weights = self.split_theta(theta)\n",
        "        for model, params in zip(self.fit_models, model_params):\n",
        "            ln += self.ln_model_prior(model, params)\n",
        "        #if self.normalize_weights:\n",
        "            #when we normalize we expect all weights to be positive\n",
        "        #we expect all weights to be positive\n",
        "        if self.strictly_positive_weights and np.any(model_weights < 0):\n",
        "            return -np.inf\n",
        "        return ln\n",
        "\n",
        "\n",
        "    def ln_model_prior(self, model, params):\n",
        "        if not model.are_params_in_bounds(params):\n",
        "            return -np.inf\n",
        "        if self.monotonicity_constraint:\n",
        "            #check for monotonicity(this obviously this is a hack, but it works for now):\n",
        "            x_mon = np.linspace(2, self.xlim, 100)\n",
        "            y_mon = model.function(x_mon, *params)\n",
        "            if np.any(np.diff(y_mon) < 0):\n",
        "                return -np.inf\n",
        "        elif self.soft_monotonicity_constraint:\n",
        "            #soft monotonicity: defined as the last value being bigger than the first one\n",
        "            x_mon = np.asarray([2, self.xlim])\n",
        "            y_mon = model.function(x_mon, *params)\n",
        "            if y_mon[0] > y_mon[-1]:\n",
        "                return -np.inf\n",
        "        ylim = model.function(self.xlim, *params)\n",
        "        #sanity check for ylim\n",
        "        if self.sanity_check_prior and not self.y_lim_sanity_check(ylim):\n",
        "            return -np.inf\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    #likelihood\n",
        "    def ln_likelihood(self, theta, x, y):\n",
        "        y_model, sigma = self.predict_given_theta(x, theta)\n",
        "\n",
        "        # We used our own logpdf function here\n",
        "        if self.recency_weighting:\n",
        "            weight = recency_weights(len(y))\n",
        "            ln_likelihood = (weight*norm.logpdf(y-y_model, loc=0, scale=sigma)).sum()\n",
        "            #ln_likelihood = (weight * logpdf(y=y-y_model, loc=0, scale=sigma)).sum()\n",
        "        else:\n",
        "            ln_likelihood = norm.logpdf(y-y_model, loc=0, scale=sigma).sum()\n",
        "            #ln_likelihood = logpdf(y=y-y_model, loc=0, scale=sigma).sum()\n",
        "\n",
        "        if np.isnan(ln_likelihood):\n",
        "            return -np.inf\n",
        "        else:\n",
        "            return ln_likelihood\n",
        "\n",
        "    def ln_prob(self, theta, x, y):\n",
        "        \"\"\"\n",
        "            posterior probability\n",
        "        \"\"\"\n",
        "        lp = self.ln_prior(theta)\n",
        "        if not np.isfinite(lp):\n",
        "            return -np.inf\n",
        "        return lp + self.ln_likelihood(theta, x, y)\n",
        "\n",
        "    def split_theta(self, theta):\n",
        "        \"\"\"\n",
        "            theta is structured as follows:\n",
        "            for each model i\n",
        "                for each model parameter j\n",
        "            theta = (theta_ij, sigma, w_i)\n",
        "        \"\"\"\n",
        "        num_models = len(self.fit_models)\n",
        "\n",
        "        model_weights = theta[-len(self.fit_models):]\n",
        "\n",
        "        all_model_params = []\n",
        "        for model in self.fit_models:\n",
        "            num_model_params = len(model.function_params)\n",
        "            model_params = theta[:num_model_params]\n",
        "            all_model_params.append(model_params)\n",
        "\n",
        "            theta = theta[num_model_params:]\n",
        "        sigma = theta[0]\n",
        "        model_weights = theta[1:]\n",
        "        assert len(model_weights) == len(self.fit_models)\n",
        "        return all_model_params, sigma, model_weights\n",
        "\n",
        "\n",
        "    def join_theta(self, model_params, sigma, model_weights):\n",
        "        #assert len(model_params) == len(model_weights)\n",
        "        theta = []\n",
        "        theta.extend(model_params)\n",
        "        theta.append(sigma)\n",
        "        theta.extend(model_weights)\n",
        "        return theta\n",
        "\n",
        "    def fit_mcmc(self, x, y):\n",
        "        #initialize in an area around the starting position\n",
        "\n",
        "        assert self.ml_params is not None\n",
        "        pos = [self.ml_params + self.rand_init_ball*np.random.randn(self.ndim) for i in range(self.nwalkers)]\n",
        "\n",
        "        if self.nthreads <= 1:\n",
        "            sampler = emcee.EnsembleSampler(self.nwalkers,\n",
        "                self.ndim,\n",
        "                self.ln_prob,\n",
        "                args=(x, y))\n",
        "        else:\n",
        "            sampler = emcee.EnsembleSampler(\n",
        "                self.nwalkers,\n",
        "                self.ndim,\n",
        "                model_ln_prob,\n",
        "                args=(self, x, y),\n",
        "                threads=self.nthreads)\n",
        "        sampler.run_mcmc(pos, self.nsamples)\n",
        "        self.mcmc_chain = sampler.chain\n",
        "\n",
        "        if self.normalize_weights:\n",
        "            self.normalize_chain_model_weights()\n",
        "\n",
        "    def normalize_chain_model_weights(self):\n",
        "        \"\"\"\n",
        "            In the chain we sample w_1,... w_i however we are interested in the model\n",
        "            probabilities p_1,... p_i\n",
        "        \"\"\"\n",
        "        model_weights_chain = self.mcmc_chain[:,:,-len(self.fit_models):]\n",
        "        model_probabilities_chain = model_weights_chain / model_weights_chain.sum(axis=2)[:,:,np.newaxis]\n",
        "        #replace in chain\n",
        "        self.mcmc_chain[:,:,-len(self.fit_models):] = model_probabilities_chain\n",
        "\n",
        "    def get_burned_in_samples(self):\n",
        "        samples = self.mcmc_chain[:, self.burn_in:, :].reshape((-1, self.ndim))\n",
        "        return samples\n",
        "\n",
        "    def print_probs(self):\n",
        "        burned_in_chain = self.get_burned_in_samples()\n",
        "        model_probabilities = burned_in_chain[:,-len(self.fit_models):]\n",
        "        print(model_probabilities.mean(axis=0))\n",
        "\n",
        "    def predict_given_theta(self, x, theta):\n",
        "        \"\"\"\n",
        "            returns y_predicted, sigma\n",
        "        \"\"\"\n",
        "        model_params, sigma, model_weights = self.split_theta(theta)\n",
        "\n",
        "        y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "\n",
        "        return y_predicted, sigma\n",
        "\n",
        "    def predict_given_params(self, x, model_params, model_weights):\n",
        "        \"\"\"\n",
        "            returns y_predicted\n",
        "        \"\"\"\n",
        "        if self.normalize_weights:\n",
        "            model_weight_sum = np.sum(model_weights)\n",
        "            model_ws = [weight/model_weight_sum for weight in model_weights]\n",
        "        else:\n",
        "            model_ws = model_weights\n",
        "\n",
        "        y_model = []\n",
        "        for model, model_w, params in zip(self.fit_models, model_ws, model_params):\n",
        "            y_model.append(model_w*model.function(x, *params))\n",
        "        y_predicted = reduce(lambda a, b: a+b, y_model)\n",
        "        return y_predicted\n",
        "\n",
        "    def prob_x_greater_than(self, x, y, theta):\n",
        "        \"\"\"\n",
        "            P(f(x) > y | Data, theta)\n",
        "        \"\"\"\n",
        "        model_params, sigma, model_weights = self.split_theta(theta)\n",
        "\n",
        "        y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "\n",
        "        # Replaced with our cdf function\n",
        "        cdf = norm.cdf(y, loc=y_predicted, scale=sigma)\n",
        "        #c = cdf(y, loc=0, scale=sigma)\n",
        "        return 1. - cdf\n",
        "\n",
        "    def posterior_prob_x_greater_than(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            P(f(x) > y | Data)\n",
        "\n",
        "            thin: only use every thin'th sample\n",
        "        \n",
        "            Posterior probability that f(x) is greater than y.\n",
        "        \"\"\"\n",
        "        assert isinstance(x, float) or isinstance(x, int)\n",
        "        assert isinstance(y, float) or isinstance(y, int)\n",
        "        probs = []\n",
        "        samples = self.get_burned_in_samples()\n",
        "        for theta in samples[::thin]:   \n",
        "            probs.append(self.prob_x_greater_than(x, y, theta))\n",
        "        return np.ma.masked_invalid(probs).mean()\n",
        "\n",
        "\n",
        "    def posterior_mean_prob_x_greater_than(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            P(E[f(x)] > E[y] | Data)\n",
        "\n",
        "            thin: only use every thin'th sample\n",
        "        \n",
        "            Posterior probability that the expected valuef(x) is greater than \n",
        "            the expected value of y.\n",
        "        \"\"\"\n",
        "        posterior_distribution = self.predictive_distribution(x, thin)\n",
        "        return masked_mean_x_greater_than(posterior_distribution, y)\n",
        "\n",
        "\n",
        "    def predictive_distribution(self, x, thin=1):\n",
        "        assert isinstance(x, float) or isinstance(x, int)\n",
        "        samples = self.get_burned_in_samples()\n",
        "        predictions = []\n",
        "        for theta in samples[::thin]:\n",
        "            model_params, sigma, model_weights = self.split_theta(theta)\n",
        "            y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "            predictions.append(y_predicted)\n",
        "        return np.asarray(predictions)\n",
        "\n",
        "    def predictive_ln_prob_distribution(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            posterior log p(y|x,D) for each sample\n",
        "        \"\"\"\n",
        "        #assert isinstance(x, float) or isinstance(x, int)\n",
        "        samples = self.get_burned_in_samples()\n",
        "        ln_probs = []\n",
        "        for theta in samples[::thin]:\n",
        "            ln_prob = self.ln_likelihood(theta, x, y)\n",
        "            ln_probs.append(ln_prob)\n",
        "        return np.asarray(ln_probs)\n",
        "\n",
        "    def posterior_ln_prob(self, x, y, thin=10):\n",
        "        \"\"\"\n",
        "            posterior log p(y|x,D)\n",
        "\n",
        "            1/S sum p(y|D,theta_s)\n",
        "            equivalent to:\n",
        "            logsumexp(log p(y|D,theta_s)) - log(S)\n",
        "        \"\"\"\n",
        "        assert not np.isscalar(x)\n",
        "        assert not np.isscalar(y)\n",
        "        x = np.asarray(x)\n",
        "        y = np.asarray(y)\n",
        "        ln_probs = self.predictive_ln_prob_distribution(x, y)\n",
        "        return logsumexp(ln_probs) - np.log(len(ln_probs))\n",
        "\n",
        "    def predict(self, x, thin=1):\n",
        "        \"\"\"\n",
        "            E[f(x)]\n",
        "        \"\"\"\n",
        "        predictions = self.predictive_distribution(x, thin)\n",
        "        return np.ma.masked_invalid(predictions).mean()\n",
        "\n",
        "    def predictive_std(self, x, thin=1):\n",
        "        \"\"\"\n",
        "           sqrt(Var[f(x)])\n",
        "        \"\"\"\n",
        "        predictions = self.predictive_distribution(x, thin)\n",
        "        return np.ma.masked_invalid(predictions).std()\n",
        "\n",
        "    def serialize(self, fname):\n",
        "        import pickle\n",
        "        pickle.dump(self, open(fname, \"wb\"))\n",
        "class MlCurveMixtureModel(object):\n",
        "    \"\"\"\n",
        "        Maximum Likelihood fit of a convex combination of curve models\n",
        "        using the Expectation Maxization algorithm.\n",
        "\n",
        "        http://www.slideshare.net/butest/lecture-18-gaussian-mixture-models-and-expectation-maximization\n",
        "        http://melodi.ee.washington.edu/people/bilmes/mypapers/em.pdf\n",
        "        http://www.igi.tugraz.at/lehre/MLA/WS07/chapter9.pdf\n",
        "\n",
        "        With Dirichlet prior:\n",
        "            ftp://tlp.limsi.fr/public/map93.pdf\n",
        "\n",
        "        Finite Mixture Model with Dirichlet Distribution\n",
        "            http://blog.datumbox.com/finite-mixture-model-based-on-dirichlet-distribution/\n",
        "\n",
        "        Variational Bayesian Gaussian Mixture Model (VBGMM)\n",
        "            http://kittipatkampa.wordpress.com/2010/10/14/variational-bayesian-gaussian-mixture-model-vbgmm/\n",
        "    \"\"\"\n",
        "    def __init__(self, ml_curve_models):\n",
        "        self.ml_curve_models = ml_curve_models\n",
        "\n",
        "    def fit(self, x, y, num_iter=1):\n",
        "        fit_models = []\n",
        "        for model in self.ml_curve_models:\n",
        "            if model.fit(x, y, start_from_default=True):\n",
        "                fit_models.append(model)\n",
        "        model_weights = [1./len(fit_models) for m in fit_models]\n",
        "        if len(fit_models) == 0:\n",
        "            return False\n",
        "        try:\n",
        "            for i in range(0, num_iter):\n",
        "                #E-step:\n",
        "                responsibilities = []\n",
        "                for model_weight, model in zip(model_weights, fit_models):\n",
        "                    #responsibilities.append(0.000001 + model_weight * model.likelihood(x, y))\n",
        "                    #responsibilities.append(0.0001 + model_weight * model.likelihood(x, y))\n",
        "                    responsibilities.append(0.000001 + model_weight * model.likelihood(x, y))\n",
        "                responsibilities = np.asarray(responsibilities)\n",
        "                #normalize:\n",
        "                responsibilities = responsibilities / responsibilities.sum(axis=0)\n",
        "\n",
        "                #M-step:\n",
        "                previous_fit_model_weights = responsibilities.mean(axis=1)\n",
        "                new_fit_models = []\n",
        "                model_weights = []\n",
        "                for model_idx, model in enumerate(fit_models):\n",
        "                    if (previous_fit_model_weights[model_idx] > 0.000001\n",
        "                        and model.fit(x, y, responsibilities[model_idx, :], start_from_default=False)):\n",
        "                        new_fit_models.append(model)\n",
        "                        #model_weights.append(previous_fit_model_weights[model_idx])\n",
        "                        model_weights.append(0.01 + previous_fit_model_weights[model_idx])\n",
        "                model_weights = np.asarray(model_weights)\n",
        "                #renormalize (in case a model couldn't be fit anymore)\n",
        "                model_weights = model_weights / model_weights.sum()\n",
        "                fit_models = new_fit_models\n",
        "                for model_weight, model in zip(model_weights, fit_models):\n",
        "                    logging.debug(\"%s %f\" % (model.function.__name__, model_weight))\n",
        "            \n",
        "            #print model_weights\n",
        "            self.model_weights = model_weights\n",
        "            self.fit_models = fit_models\n",
        "            return True\n",
        "        except: \n",
        "            return False\n",
        "\n",
        "    def predict(self, x):\n",
        "        y_predicted = None\n",
        "        for model_weight, model in zip(self.model_weights, self.fit_models):\n",
        "            if y_predicted is None:\n",
        "                y_predicted = model_weight * model.predict(x)\n",
        "            else:\n",
        "                y_predicted += model_weight * model.predict(x)\n",
        "        return y_predicted\n",
        "\n",
        "\n",
        "class MCMCCurveMixtureModel(object):\n",
        "\n",
        "    def __init__(self,\n",
        "            ml_curve_models,\n",
        "            xlim,\n",
        "            burn_in=600,\n",
        "            nwalkers=80,\n",
        "            nsamples=5000,\n",
        "            monotonicity_constraint=True,\n",
        "            soft_monotonicity_constraint=False,\n",
        "            nthreads=1,\n",
        "            recency_weighting=True):\n",
        "        \"\"\"\n",
        "            xlim: the point on the x axis we eventually want to make predictions for.\n",
        "        \"\"\"\n",
        "        self.ml_curve_models = ml_curve_models\n",
        "        self.ml_curve_mixture_model = MlCurveMixtureModel(ml_curve_models)\n",
        "        self.xlim = xlim\n",
        "        self.burn_in = burn_in\n",
        "        self.nwalkers = nwalkers\n",
        "        self.nsamples = nsamples\n",
        "        assert not (monotonicity_constraint and soft_monotonicity_constraint), \"choose either the monotonicity_constraint or the soft_monotonicity_constraint, but not both\"\n",
        "        self.monotonicity_constraint = monotonicity_constraint\n",
        "        self.soft_monotonicity_constraint = soft_monotonicity_constraint\n",
        "        self.nthreads = nthreads\n",
        "        self.recency_weighting = recency_weighting\n",
        "        #the constant used for initializing the parameters in a ball around the ML parameters\n",
        "        self.rand_init_ball = 1e-6\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        if self.fit_ml_individual(x, y):\n",
        "            #run MCMC:\n",
        "            self.fit_mcmc(x, y)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"fit_ml_individual failed\")\n",
        "            return False\n",
        "\n",
        "    def fit_ml_individual(self, x, y):\n",
        "        \"\"\"\n",
        "            Do a ML fit for each model individually and then another ML fit for the combination of models.\n",
        "        \"\"\"\n",
        "        if self.ml_curve_mixture_model.fit(x, y):\n",
        "            model_weights = self.ml_curve_mixture_model.model_weights\n",
        "            self.fit_models = self.ml_curve_mixture_model.fit_models\n",
        "        else:\n",
        "            self.fit_models = []\n",
        "            for model in self.ml_curve_models:\n",
        "                if model.fit(x, y):\n",
        "                    if np.isfinite(self.ln_model_prior(model, model.ml_params)):\n",
        "                        self.fit_models.append(model)\n",
        "                    else:\n",
        "                        print(\"ML fit of model %s is not supported by prior.\" % model.function.__name__)\n",
        "            model_weights = [10. for model in self.fit_models]\n",
        "            if len(self.fit_models) == 0:\n",
        "                return False\n",
        "\n",
        "        #build joint ml estimated parameter vector\n",
        "        all_model_params = []\n",
        "        for model in self.fit_models:\n",
        "            all_model_params.extend(model.ml_params)\n",
        "        print(\"model weights: \", model_weights)\n",
        "        self.ml_params = self.join_theta(all_model_params, model_weights)\n",
        "        self.ndim = len(self.ml_params)\n",
        "        return True\n",
        "\n",
        "    #priors\n",
        "    def ln_prior(self, theta):\n",
        "        ln = 0\n",
        "        model_thetas, model_weights = self.split_theta(theta)\n",
        "        for model, theta in zip(self.fit_models, model_thetas):\n",
        "            ln += self.ln_model_prior(model, theta)\n",
        "        #if self.normalize_weights:\n",
        "            #when we normalize we expect all weights to be positive\n",
        "        #we expect all weights to be positive\n",
        "        if np.any(model_weights < 0):\n",
        "            return -np.inf\n",
        "        return ln\n",
        "\n",
        "    def ln_model_prior(self, model, theta):\n",
        "        if not model.are_params_in_bounds(theta):\n",
        "            return -np.inf\n",
        "        if self.monotonicity_constraint:\n",
        "            #check for monotonicity(this obviously this is a hack, but it works for now):\n",
        "            x_mon = np.linspace(2, self.xlim, 100)\n",
        "            params, sigma = model.split_theta_to_array(theta)\n",
        "            y_mon = model.function(x_mon, *params)\n",
        "            if np.any(np.diff(y_mon) < 0):\n",
        "                return -np.inf\n",
        "        elif self.soft_monotonicity_constraint:\n",
        "            #soft monotonicity: defined as the last value being bigger than the first one\n",
        "            x_mon = np.asarray([2, self.xlim])\n",
        "            y_mon = model.function(x_mon, *params)\n",
        "            if y_mon[0] > y_mon[-1]:\n",
        "                return -np.inf\n",
        "        return 0.0\n",
        "\n",
        "    #likelihood\n",
        "    def ln_likelihood(self, theta, x, y):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        sample_weights = None\n",
        "        if self.recency_weighting:\n",
        "            sample_weights = [10**(1./len(y))] * len(y)\n",
        "            sample_weights = sample_weights**(np.arange(0, len(y)))\n",
        "\n",
        "        model_thetas, model_weights = self.split_theta(theta)\n",
        "        #normalize the weights\n",
        "        model_weight_sum = np.sum(model_weights)\n",
        "        model_weights = [weight/model_weight_sum for weight in model_weights]\n",
        "\n",
        "        ln_likelihoods = []\n",
        "        for model, model_theta, model_weight in zip(self.fit_models, model_thetas, model_weights):\n",
        "            ln_likelihood = np.log(model_weight)\n",
        "            params, sigma = model.split_theta_to_array(model_theta)\n",
        "            y_model = model.function(x, *params)\n",
        "            if sample_weights is None:\n",
        "                ln_likelihood += norm.logpdf(y-y_model, loc=0, scale=sigma).sum()\n",
        "            else:\n",
        "                ln_likelihood += (sample_weights*norm.logpdf(y-y_model, loc=0, scale=sigma)).sum()\n",
        "\n",
        "            ln_likelihoods.append(ln_likelihood)\n",
        "\n",
        "        if np.any(np.isnan(ln_likelihoods)):\n",
        "            return -np.inf\n",
        "        else:\n",
        "            return logsumexp(ln_likelihoods)\n",
        "\n",
        "    def ln_prob(self, theta, x, y):\n",
        "        \"\"\"\n",
        "            posterior probability\n",
        "        \"\"\"\n",
        "        lp = self.ln_prior(theta)\n",
        "        if not np.isfinite(lp):\n",
        "            return -np.inf\n",
        "        return lp + self.ln_likelihood(theta, x, y)\n",
        "\n",
        "    def split_theta(self, theta):\n",
        "        \"\"\"\n",
        "            theta is structured as follows:\n",
        "            for each model i\n",
        "                for each model parameter j\n",
        "            theta = (theta_ij, sigma, w_i)\n",
        "        \"\"\"\n",
        "        num_models = len(self.fit_models)\n",
        "\n",
        "        model_weights = theta[-len(self.fit_models):]\n",
        "\n",
        "        all_model_params = []\n",
        "        for model in self.fit_models:\n",
        "            num_model_params = len(model.all_param_names)\n",
        "            model_params = theta[:num_model_params]\n",
        "            all_model_params.append(model_params)\n",
        "\n",
        "            theta = theta[num_model_params:]\n",
        "        model_weights = theta\n",
        "        assert len(model_weights) == len(self.fit_models)\n",
        "        return all_model_params, model_weights\n",
        "\n",
        "\n",
        "    def join_theta(self, model_params, model_weights):\n",
        "        #assert len(model_params) == len(model_weights)\n",
        "        theta = []\n",
        "        theta.extend(model_params)\n",
        "        theta.extend(model_weights)\n",
        "        return theta\n",
        "\n",
        "    def fit_mcmc(self, x, y):\n",
        "        #initialize in an area around the starting position\n",
        "\n",
        "        assert self.ml_params is not None\n",
        "        pos = [self.ml_params + self.rand_init_ball*np.random.randn(self.ndim) for i in range(self.nwalkers)]\n",
        "\n",
        "        if self.nthreads <= 1:\n",
        "            sampler = emcee.EnsembleSampler(self.nwalkers,\n",
        "                self.ndim,\n",
        "                self.ln_prob,\n",
        "                args=(x, y))\n",
        "        else:\n",
        "            sampler = emcee.EnsembleSampler(\n",
        "                self.nwalkers,\n",
        "                self.ndim,\n",
        "                model_ln_prob,\n",
        "                args=(self, x, y),\n",
        "                threads=self.nthreads)\n",
        "        sampler.run_mcmc(pos, self.nsamples)\n",
        "        self.mcmc_chain = sampler.chain\n",
        "\n",
        "        #we normalize the weights in the chain model, so that the trace plot make more sense\n",
        "        self.normalize_chain_model_weights()\n",
        "\n",
        "    def normalize_chain_model_weights(self):\n",
        "        \"\"\"\n",
        "            In the chain we sample w_1,... w_i however we are interested in the model\n",
        "            probabilities p_1,... p_i\n",
        "        \"\"\"\n",
        "        model_weights_chain = self.mcmc_chain[:,:,-len(self.fit_models):]\n",
        "        model_probabilities_chain = model_weights_chain / model_weights_chain.sum(axis=2)[:,:,np.newaxis]\n",
        "        #replace in chain\n",
        "        self.mcmc_chain[:,:,-len(self.fit_models):] = model_probabilities_chain\n",
        "\n",
        "    def get_burned_in_samples(self):\n",
        "        samples = self.mcmc_chain[:, self.burn_in:, :].reshape((-1, self.ndim))\n",
        "        return samples\n",
        "\n",
        "    def print_probs(self):\n",
        "        burned_in_chain = self.get_burned_in_samples()\n",
        "        model_probabilities = burned_in_chain[:,-len(self.fit_models):]\n",
        "        print(model_probabilities.mean(axis=0))\n",
        "\n",
        "    def predict_given_theta(self, x, theta):\n",
        "        \"\"\"\n",
        "            returns y_predicted, sigma\n",
        "        \"\"\"\n",
        "        model_params, model_weights = self.split_theta(theta)\n",
        "\n",
        "        y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "\n",
        "        return y_predicted\n",
        "\n",
        "    def predict_given_params(self, x, model_thetas, model_weights):\n",
        "        \"\"\"\n",
        "            returns y_predicted\n",
        "        \"\"\"\n",
        "        #normalize the weights\n",
        "        model_weight_sum = np.sum(model_weights)\n",
        "        model_ws = [weight/model_weight_sum for weight in model_weights]\n",
        "\n",
        "        y_model = []\n",
        "        for model, model_w, theta in zip(self.fit_models, model_ws, model_thetas):\n",
        "            params, sigma = model.split_theta_to_array(theta)\n",
        "            y_model.append(model_w*model.function(x, *params))\n",
        "        y_predicted = reduce(lambda a, b: a+b, y_model)\n",
        "        return y_predicted\n",
        "\n",
        "    def prob_x_greater_than(self, x, y, theta):\n",
        "        \"\"\"\n",
        "            P(f(x) > y | Data, theta)\n",
        "        \"\"\"\n",
        "        model_params, model_weights = self.split_theta(theta)\n",
        "\n",
        "        y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "\n",
        "        cdf = norm.cdf(y, loc=y_predicted, scale=sigma)\n",
        "        return 1. - cdf\n",
        "\n",
        "    def posterior_prob_x_greater_than(self, x, y, thin=1):\n",
        "        \"\"\"\n",
        "            P(f(x) > y | Data)\n",
        "\n",
        "            thin: only use every thin'th sample\n",
        "        \n",
        "            Posterior probability that f(x) is greater than y.\n",
        "        \"\"\"\n",
        "        assert isinstance(x, float) or isinstance(x, int)\n",
        "        assert isinstance(y, float) or isinstance(y, int)\n",
        "        probs = []\n",
        "        samples = self.get_burned_in_samples()\n",
        "        for theta in samples[::thin]:   \n",
        "            probs.append(self.prob_x_greater_than(x, y, theta))\n",
        "\n",
        "        return np.ma.masked_invalid(probs).mean()\n",
        "\n",
        "    def predictive_distribution(self, x, thin=1):\n",
        "        assert isinstance(x, float) or isinstance(x, int)\n",
        "        samples = self.get_burned_in_samples()\n",
        "        predictions = []\n",
        "        for theta in samples[::thin]:\n",
        "            model_params, sigma, model_weights = self.split_theta(theta)\n",
        "            y_predicted = self.predict_given_params(x, model_params, model_weights)\n",
        "            predictions.append(y_predicted)\n",
        "        return np.asarray(predictions)\n",
        "\n",
        "    def predict(self, x, thin=1):\n",
        "        \"\"\"\n",
        "            E[f(x)]\n",
        "        \"\"\"\n",
        "        predictions = self.predictive_distribution(x, thin)\n",
        "        return np.ma.masked_invalid(predictions).mean()\n",
        "\n",
        "    def predictive_std(self, x, thin=1):\n",
        "        \"\"\"\n",
        "            sqrt(Var[f(x)])\n",
        "        \"\"\"\n",
        "        predictions = self.predictive_distribution(x, thin)\n",
        "        return np.ma.masked_invalid(predictions).std()\n",
        "\n",
        "    def serialize(self, fname):\n",
        "        import pickle\n",
        "        pickle.dump(self, open(fname, \"wb\"))\n"
      ],
      "metadata": {
        "id": "VtSKEYJxlGgy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Curve Model**"
      ],
      "metadata": {
        "id": "7iz-HhzfmFDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(model, x_train, y_train):\n",
        "    success = model.fit(x_train, y_train)\n",
        "    return (success, model)\n",
        "\n",
        "def model_log_likelihood(model, x_test, y_test):\n",
        "    return model.posterior_log_likelihood(x_test, y_test)\n",
        "\n",
        "def model_posterior_prob_x_greater_than(model, x, y):\n",
        "    return model.posterior_prob_x_greater_than(x, y)\n",
        "\n",
        "def train_test_split(x, y, train_fraction):\n",
        "    #split into train/test\n",
        "    if train_fraction > 0.99:\n",
        "        x_train = x\n",
        "        y_train = y\n",
        "        x_test = x\n",
        "        y_test = y\n",
        "    else:\n",
        "        num_train = int(train_fraction * len(x))\n",
        "        x_train = x[:num_train]\n",
        "        y_train = y[:num_train]\n",
        "        x_test = x[num_train:]\n",
        "        y_test = y[num_train:]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "class Ensemble(object):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models, map=map):\n",
        "        \"\"\"\n",
        "            models: ensemble models\n",
        "            map: map function, if multiprocessing is desired\n",
        "        \"\"\"\n",
        "        self.all_models = models\n",
        "        self._map = map\n",
        "        self.fit_models = []\n",
        "\n",
        "\n",
        "class CurveModelEnsemble(Ensemble):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models, map=map):\n",
        "        super(CurveModelEnsemble, self).__init__(models, map)\n",
        "\n",
        "    def fit(self, x, y, train_fraction=0.8):\n",
        "        assert len(x) == len(y)\n",
        "\n",
        "        model_log_likelihoods = []\n",
        "        self.fit_models = []\n",
        "\n",
        "        x_train, y_train, x_test, y_test = train_test_split(x, y, train_fraction)\n",
        "\n",
        "        fit_result = self._map(\n",
        "            partial(fit_model, x_train=x_train, y_train=y_train),\n",
        "            self.all_models)\n",
        "        for success, model in fit_result:\n",
        "            if success:\n",
        "                self.fit_models.append(model)\n",
        "\n",
        "        if len(self.fit_models) == 0:\n",
        "            logging.warn(\"EnsembleCurveModel couldn't fit any models!!!\")\n",
        "            return False\n",
        "\n",
        "        model_log_likelihoods = self._map(\n",
        "            partial(model_log_likelihood, x_test=x_test, y_test=y_test),\n",
        "            self.fit_models)\n",
        "\n",
        "        normalizing_constant = logsumexp(model_log_likelihoods)\n",
        "\n",
        "        self.model_probabilities = [np.exp(log_lik - normalizing_constant) for log_lik in model_log_likelihoods]\n",
        "        return True\n",
        "\n",
        "    def posterior_prob_x_greater_than(self, x, y):\n",
        "        \"\"\"\n",
        "            The probability under the models that a value y is exceeded at position x.\n",
        "            IMPORTANT: if all models fail, by definition the probability is 1.0 and NOT 0.0\n",
        "        \"\"\"\n",
        "        if len(self.fit_models) == 0:\n",
        "            return 1.0\n",
        "\n",
        "        models_prob_x_greater_than = model_log_likelihoods = self._map(\n",
        "            partial(model_posterior_prob_x_greater_than, x=x, y=y),\n",
        "            self.fit_models)\n",
        "\n",
        "        overall_prob = 0\n",
        "        for prob_x_greater_than, model_prob in zip(models_prob_x_greater_than, self.model_probabilities):\n",
        "            overall_prob += model_prob * prob_x_greater_than\n",
        "        return overall_prob\n",
        "\n",
        "    def posterior_log_likelihood(self, x, y):\n",
        "        log_liks = []\n",
        "        for model, model_prob in zip(self.fit_models, self.model_probabilities):\n",
        "            log_lik = model.posterior_log_likelihood(x, y)\n",
        "            log_liks.append(np.log(model_prob) + log_lik)\n",
        "        return logsumexp(log_liks)\n",
        "\n",
        "    def predict(self, x):\n",
        "        if np.isscalar(x):\n",
        "            y = 0\n",
        "        else:\n",
        "            y = np.zeros(x.shape)\n",
        "        for model, model_prob in zip(self.fit_models, self.model_probabilities):\n",
        "            y += model_prob * model.predict(x)\n",
        "        return y\n",
        "\n",
        "    def __str__(self):\n",
        "        ret = []\n",
        "        model_names = [model.function.__name__ for model in self.fit_models]\n",
        "        for model_prob, model_name in zip(self.model_probabilities, model_names):\n",
        "            ret.append(\"%s: %f\\n\" % (model_name, model_prob))\n",
        "        return \"\".join(ret)\n",
        "\n",
        "    def serialize(self, fname):\n",
        "        import pickle\n",
        "        map_tmp = self._map\n",
        "        self._map = None\n",
        "        pickle.dump(self, open(fname, \"wb\"))\n",
        "        self._map = map_tmp\n",
        "\n",
        "\n",
        "\n",
        "class CurveEnsemble(Ensemble):\n",
        "    \"\"\"\n",
        "        1. MCMC fitting\n",
        "        2. Now take each theta as a model\n",
        "        3. Make predictions as an weighted average of those models\n",
        "            The weight is the (normalized) likelihood of some held out validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models, map=map):\n",
        "        super(CurveEnsemble, self).__init__(models, map)\n",
        "\n",
        "    def fit(self, x, y, train_fraction=0.8):\n",
        "        assert len(x) == len(y)\n",
        "\n",
        "        model_log_likelihoods = []\n",
        "        self.fit_models = []\n",
        "\n",
        "        x_train, y_train, x_test, y_test = train_test_split(x, y, train_fraction)\n",
        "\n",
        "        fit_result = self._map(\n",
        "            partial(fit_model, x_train=x_train, y_train=y_train),\n",
        "            self.all_models)\n",
        "        for success, model in fit_result:\n",
        "            if success:\n",
        "                self.fit_models.append(model)\n",
        "\n",
        "        if len(self.fit_models) == 0:\n",
        "            logging.warn(\"EnsembleCurveModel couldn't fit any models!!!\")\n",
        "            return\n",
        "\n",
        "        #Now we interpret each theta as a separate model\n",
        "        #TODO: parallelize!\n",
        "        all_log_likelihoods = []\n",
        "        for model in self.fit_models:\n",
        "            model_log_likelihoods = []\n",
        "            thetas = model.get_burned_in_samples()\n",
        "            for theta_idx in range(thetas.shape[0]):\n",
        "                theta = thetas[theta_idx,:]\n",
        "                log_likelihood = model.ln_likelihood(theta, x_test, y_test)\n",
        "                model_log_likelihoods.append(log_likelihood)\n",
        "            all_log_likelihoods.append(model_log_likelihoods)\n",
        "\n",
        "        self.model_theta_probabilities = [np.exp(model_log_likelihoods - logsumexp(model_log_likelihoods)) for model_log_likelihoods in all_log_likelihoods]\n",
        "\n",
        "        normalizing_constant = logsumexp(all_log_likelihoods)\n",
        "\n",
        "        normalize = lambda log_lik: np.exp(log_lik - normalizing_constant)\n",
        "\n",
        "        self.all_model_probabilities = [[normalize(log_lik) for log_lik in model_log_likelihoods] for model_log_likelihoods in all_log_likelihoods]\n",
        "\n",
        "        #sum up on a per model family basis:\n",
        "        self.model_probabilities = [sum(model_probabilities) for model_probabilities in self.all_model_probabilities]\n",
        "\n",
        "\n",
        "    def posterior_prob_x_greater_than(self, x, y):\n",
        "        \"\"\"\n",
        "            The probability under the models that a value y is exceeded at position x.\n",
        "            IMPORTANT: if all models fail, by definition the probability is 1.0 and NOT 0.0\n",
        "        \"\"\"\n",
        "        if len(self.fit_models) == 0:\n",
        "            return 1.0\n",
        "\n",
        "        overall_prob = 0\n",
        "        for model, theta_model_probabilities in zip(self.fit_models, self.all_model_probabilities):\n",
        "            thetas = model.get_burned_in_samples()\n",
        "            for theta_idx, theta_model_probability in zip(range(thetas.shape[0]), theta_model_probabilities):\n",
        "                theta = thetas[theta_idx, :]\n",
        "                overall_prob += theta_model_probability * model.prob_x_greater_than(x, y, theta)\n",
        "        return overall_prob\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        if np.isscalar(x):\n",
        "            y = 0\n",
        "        else:\n",
        "            y = np.zeros(x.shape)\n",
        "        #TOOD: implement!!\n",
        "        return y\n",
        "\n",
        "    def __str__(self):\n",
        "        ret = []\n",
        "        model_names = [model.function.__name__ for model in self.fit_models]\n",
        "        for model_prob, model_name in zip(self.model_probabilities, model_names):\n",
        "            ret.append(\"%s: %f\\n\" % (model_name, model_prob))\n",
        "        return \"\".join(ret)"
      ],
      "metadata": {
        "id": "TlNppa5-mODy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MCMC Model Plotter**"
      ],
      "metadata": {
        "id": "tjwItdrymbo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greek_label_mapping(oldlabels):\n",
        "    labels = []\n",
        "    for param_name in oldlabels:\n",
        "        if param_name in [\"alpha\", \"beta\", \"delta\", \"sigma\"]:#\"kappa\", \n",
        "            labels.append(\"$\\%s$\" % param_name)\n",
        "        else:\n",
        "            labels.append(\"$%s$\" % param_name)\n",
        "    return labels\n",
        "\n",
        "class MCMCCurveModelPlotter(object):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        \n",
        "    def trace_plot(self,rasterized=False):\n",
        "        num_plots = len(self.model.all_param_names)\n",
        "        \n",
        "        fig, axes = subplots(num_plots, 1, sharex=True, figsize=(8, 9))\n",
        "        \n",
        "        for idx, param_name in enumerate(self.model.all_param_names):\n",
        "        \n",
        "            axes[idx].plot(self.model.mcmc_chain[:, :, idx].T, color=\"k\",\n",
        "                alpha=0.4, rasterized=rasterized)\n",
        "            axes[idx].yaxis.set_major_locator(MaxNLocator(5))\n",
        "            #axes[0].axhline(m_true, color=\"#888888\", lw=2)\n",
        "            axes[idx].set_ylabel(\"$%s$\" % param_name)\n",
        "            \n",
        "            if idx == num_plots-1:\n",
        "                axes[idx].set_xlabel(\"step number\")\n",
        "        \n",
        "        tight_layout(h_pad=0.0)\n",
        "        \n",
        "\n",
        "    def triangle_plot(self, labels=None, truths=None):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "        if labels is None:\n",
        "            labels = greek_label_mapping(self.model.all_param_names)\n",
        "        fig = triangle.corner(samples,\n",
        "            labels=labels,\n",
        "            truths=truths)\n",
        "\n",
        "    def posterior_sample_plot(self, x, y=None, vline=None,\n",
        "            xaxislabel=\"$x$\", yaxislabel=\"$y$\", alpha=0.3,\n",
        "            label=\"\", color=\"k\", x_axis_scale=0.1, nsamples=50,\n",
        "            plot_ml_estimate=False, ml_estimate_color=\"#4682b4\",\n",
        "            rasterized=False):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "\n",
        "        x_plot = x_axis_scale*np.asarray(x)\n",
        "\n",
        "        if y is not None:\n",
        "            plot(x_plot, y, color=\"r\", lw=2, alpha=0.8, rasterized=rasterized)\n",
        "\n",
        "        # Plot some samples onto the data.\n",
        "        for idx, theta in enumerate(samples[np.random.randint(len(samples), size=nsamples)]):\n",
        "        #for idx, theta in enumerate(samples):\n",
        "            #print theta\n",
        "            predictive_mu, sigma = self.model.predict_given_theta(x, theta)\n",
        "\n",
        "            if idx == 0:\n",
        "                plot(x_plot, predictive_mu, color=color, alpha=alpha, label=label, rasterized=rasterized)\n",
        "            else:\n",
        "                plot(x_plot, predictive_mu, color=color, alpha=alpha, rasterized=rasterized)\n",
        "            fill_between(x_plot, predictive_mu-sigma, predictive_mu+sigma, color=color, alpha=0.01, rasterized=rasterized)\n",
        "\n",
        "        if plot_ml_estimate:\n",
        "            ml_theta = self.model.ml_curve_model.ml_params\n",
        "            predictive_mu, sigma = self.model.predict_given_theta(x, ml_theta)\n",
        "            plot(x_plot, predictive_mu, alpha=1.0, color=ml_estimate_color, lw=3, rasterized=rasterized)\n",
        "            fill_between(x_plot, predictive_mu-sigma, predictive_mu+sigma, color=ml_estimate_color, alpha=0.3, rasterized=rasterized)\n",
        "\n",
        "        \n",
        "        if vline is not None:\n",
        "            axvline(x_axis_scale*vline, color=\"k\")\n",
        "        ylim(0, 1)\n",
        "        xlabel(xaxislabel)\n",
        "        ylabel(yaxislabel)   \n",
        "        tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "    def predictive_density_plot(self, x):\n",
        "        x_lin =  linspace(0., 1., 100)\n",
        "        plot(x_lin, self.model.predictive_density(x, x_lin));\n",
        "\n",
        "\n",
        "\n",
        "class MCMCCurveModelCombinationPlotter(object):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.colors = ['r', 'g', 'b', 'y', 'cyan', 'magenta', 'Chocolate', 'Crimson', 'DeepSkyBlue', 'Khaki']\n",
        "        \n",
        "    def trace_plot(self, figsize=None):\n",
        "        num_plots = self.model.ndim\n",
        "        \n",
        "        if figsize is None:\n",
        "            figsize = (8, num_plots)\n",
        "        fig, axes = subplots(num_plots, 1, sharex=True, figsize=figsize)\n",
        "\n",
        "        labels = [\"$%s$\" % (param_name) for model in self.model.fit_models for param_name in model.function_params]\n",
        "        titles = [\"%s\" % (model.name) for model in self.model.fit_models for param_name in model.function_params]\n",
        "        labels.append(\"$sigma$\")\n",
        "        titles.append(\"\")\n",
        "        labels.extend([\"$w%d$\" % idx for idx in range(len(self.model.fit_models))])\n",
        "        titles.extend([\"%s weight\" % model.name for model in self.model.fit_models])\n",
        "\n",
        "        for idx, label, title in zip(range(self.model.ndim), labels, titles):\n",
        "            axes[idx].set_title(title)\n",
        "            axes[idx].set_ylabel(label)\n",
        "            axes[idx].plot(self.model.mcmc_chain[:, :, idx].T, color=\"k\", alpha=0.4)\n",
        "            axes[idx].yaxis.set_major_locator(MaxNLocator(5))\n",
        "            #axes[0].axhline(m_true, color=\"#888888\", lw=2)\n",
        "            \n",
        "            if idx == num_plots-1:\n",
        "                axes[idx].set_xlabel(\"step number\")\n",
        "        \n",
        "        tight_layout(h_pad=0.0)\n",
        "        \n",
        "\n",
        "    def triangle_plot(self, labels=None):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "        if labels is None:\n",
        "            labels = [\"$%s$\" % (param_name) for model in self.model.fit_models for param_name in model.function_params]\n",
        "            labels.append(\"$sigma$\")\n",
        "            labels.extend([\"$w%d$\" % idx for idx in range(len(self.model.fit_models))])\n",
        "            labels = greek_label_mapping(labels)\n",
        "        fig = triangle.corner(samples,\n",
        "            labels=labels)\n",
        "\n",
        "    def weights_triangle_plot(self, labels=None, thin=1):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "        if labels is None:\n",
        "            labels = [\"w%d\" % idx for idx in range(len(self.model.fit_models))]\n",
        "            #labels = greek_label_mapping(labels)\n",
        "            print(labels)\n",
        "        fig = triangle.corner(\n",
        "            samples[::thin,-len(self.model.fit_models):])#,\n",
        "            #labels=labels)\n",
        "\n",
        "    def weights_triangle_plot(self, labels=None, thin=1):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "        if labels is None:\n",
        "            labels = [\"w%d\" % idx for idx in range(len(self.model.fit_models))]\n",
        "            #labels = greek_label_mapping(labels)\n",
        "            print(labels)\n",
        "        fig = triangle.corner(\n",
        "            samples[::thin,-len(self.model.fit_models):])#,\n",
        "            #labels=labels)\n",
        "\n",
        "    def posterior_plot(self, *args, **kwargs):\n",
        "        self.posterior_sample_plot(*args, **kwargs)\n",
        "\n",
        "    def posterior_sample_plot(self, x, y=None, vline=None, alpha=0.1, label=\"\",\n",
        "            xaxislabel=\"$x$\", yaxislabel=\"$y$\", color=\"k\", x_axis_scale=0.1,\n",
        "            x_lim=None, plot_individual=False, y_plot_lw=2,\n",
        "            rasterized=False):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "\n",
        "        if x_lim is None:\n",
        "            x_predict = x\n",
        "        else:\n",
        "            x_predict = np.arange(1, x_lim, 1)\n",
        "\n",
        "        x = x_axis_scale*x\n",
        "        x_plot = x_axis_scale*np.asarray(x_predict)\n",
        "\n",
        "        # Plot some samples onto the data.\n",
        "        for idx, theta in enumerate(samples[np.random.randint(len(samples), size=100)]):\n",
        "            predictive_mu, sigma = self.model.predict_given_theta(x_predict, theta)\n",
        "\n",
        "            if idx == 0:\n",
        "                plot(x_plot, predictive_mu, color=color, alpha=alpha, label=label, rasterized=rasterized)\n",
        "            else:\n",
        "                plot(x_plot, predictive_mu, color=color, alpha=alpha, rasterized=rasterized)\n",
        "\n",
        "            fill_between(x_plot, predictive_mu-2*sigma, predictive_mu+2*sigma, color=color,\n",
        "                rasterized=rasterized, alpha=0.01)\n",
        "            if not plot_individual:\n",
        "                continue\n",
        "            #plot the contributions of the individual models:\n",
        "            model_params, sigma, model_weights = self.model.split_theta(theta)\n",
        "            if self.model.normalize_weights:\n",
        "                model_weight_sum = np.sum(model_weights)\n",
        "                model_probs = [weight/model_weight_sum for weight in model_weights]\n",
        "            else:\n",
        "                model_probs = model_weights\n",
        "            for fit_model, model_color, model_prob, params in zip(self.model.fit_models, self.colors, model_probs, model_params):\n",
        "                #if fit_model.function.__name__ != \"ilog2\":\n",
        "                #    continue\n",
        "                try:\n",
        "                    sub_model_predictive_mu = fit_model.function(x, *params)\n",
        "                    #plot(x_plot, model_prob * sub_model_predictive_mu, color=model_color, alpha=alpha)\n",
        "                    plot(x_plot, sub_model_predictive_mu, color=model_color, alpha=alpha)\n",
        "                except:\n",
        "                    print(\"error with model: \", fit_model.function.__name__)\n",
        "\n",
        "        if y is not None:\n",
        "            plot(x, y, color=\"r\", lw=y_plot_lw, alpha=0.8, label=\"data\")\n",
        "        if vline is not None:\n",
        "            axvline(x_axis_scale*vline, color=\"k\")\n",
        "        ylim(0, 1)\n",
        "        xlabel(xaxislabel)\n",
        "        ylabel(yaxislabel)   \n",
        "        tight_layout()\n",
        "\n",
        "    def ml_single_models_plot(self, x, y, vline=None, x_axis_scale=0.1):\n",
        "        lin_comb = None\n",
        "        x_plot = x_axis_scale*np.asarray(x)\n",
        "        for m in self.model.fit_models:\n",
        "            plot(x_plot, m.predict(x), alpha=0.7, label=m.function.__name__, lw=2)\n",
        "            model_weight = 1. / len(self.model.fit_models)\n",
        "            if lin_comb is None:\n",
        "                lin_comb = model_weight * m.predict(x)\n",
        "            else:\n",
        "                lin_comb += model_weight * m.predict(x)\n",
        "        plot(x_plot, m.predict(x), alpha=0.7, label=\"linear combination\", lw=2)\n",
        "\n",
        "        plot(x_plot, y, color=\"r\", lw=2, alpha=0.8)\n",
        "        if vline is not None:\n",
        "            axvline(x_axis_scale*vline, color=\"k\")\n",
        "        ylim(0, 1)\n",
        "        legend(loc=4)\n",
        "        xlabel(\"$x$\")\n",
        "        ylabel(\"$y$\")   \n",
        "        tight_layout()\n",
        "\n",
        "\n",
        "    def predictive_density_plot(self, x):\n",
        "        x_lin =  linspace(0., 1., 100)\n",
        "        plot(x_lin, self.model.predictive_density(x, x_lin));\n",
        "\n",
        "\n",
        "class MCMCCurveMixtureModelPlotter(object):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.colors = ['r', 'g', 'b', 'y', 'cyan', 'magenta', 'Chocolate', 'Crimson', 'DeepSkyBlue', 'Khaki']\n",
        "        \n",
        "    def trace_plot(self, figsize=None):\n",
        "        num_plots = self.model.ndim\n",
        "        \n",
        "        if figsize is None:\n",
        "            figsize = (8, num_plots)\n",
        "        fig, axes = subplots(num_plots, 1, sharex=True, figsize=figsize)\n",
        "\n",
        "        labels = [\"$%s$\" % (param_name) for model in self.model.fit_models for param_name in model.all_param_names]\n",
        "        titles = [\"%s\" % (model.name) for model in self.model.fit_models for param_name in model.all_param_names]\n",
        "        labels.extend([\"$w%d$\" % idx for idx in range(len(self.model.fit_models))])\n",
        "        titles.extend([\"%s weight\" % model.name for model in self.model.fit_models])\n",
        "\n",
        "        for idx, label, title in zip(range(self.model.ndim), labels, titles):\n",
        "            axes[idx].set_title(title)\n",
        "            axes[idx].set_ylabel(label)\n",
        "            axes[idx].plot(self.model.mcmc_chain[:, :, idx].T, color=\"k\", alpha=0.4)\n",
        "            axes[idx].yaxis.set_major_locator(MaxNLocator(5))\n",
        "            #axes[0].axhline(m_true, color=\"#888888\", lw=2)\n",
        "            \n",
        "            if idx == num_plots-1:\n",
        "                axes[idx].set_xlabel(\"step number\")\n",
        "        \n",
        "        tight_layout(h_pad=0.0)\n",
        "        \n",
        "\n",
        "    def triangle_plot(self):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "        \n",
        "        labels = [\"$%s$\" % param_name  for model in self.model.fit_models for param_name in model.all_param_names]\n",
        "        labels.extend([\"$%s$\" for model in self.model.fit_models])\n",
        "        fig = triangle.corner(samples, labels=labels)\n",
        "\n",
        "\n",
        "    def posterior_plot(self, *args, **kwargs):\n",
        "        self.posterior_sample_plot(*args, **kwargs)\n",
        "\n",
        "    def posterior_sample_plot(self, x, y, vline=None, alpha=0.8, label=\"\", color=\"k\", x_axis_scale=0.1):\n",
        "        samples = self.model.get_burned_in_samples()\n",
        "\n",
        "        x_plot = x_axis_scale*np.asarray(x)\n",
        "\n",
        "        # Plot some samples onto the data.\n",
        "        for idx, theta in enumerate(samples[np.random.randint(len(samples), size=100)]):\n",
        "            predictive_mu = self.model.predict_given_theta(x, theta)\n",
        "\n",
        "            if idx == 0:\n",
        "                plot(x_plot, predictive_mu, color=color, alpha=alpha, label=label)\n",
        "            else:\n",
        "                plot(x_plot, predictive_mu, color=color, alpha=alpha)\n",
        "\n",
        "        plot(x_plot, y, color=\"r\", lw=2, alpha=0.8)\n",
        "        if vline is not None:\n",
        "            axvline(x_axis_scale*vline, color=\"k\")\n",
        "        ylim(0, 1)\n",
        "        xlabel(\"$x$\")\n",
        "        ylabel(\"$y$\")   \n",
        "        tight_layout()\n",
        "\n",
        "\n",
        "class EnsemblePlotter(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 ensemble_curve_model,\n",
        "                 colors=['r', 'g', 'b', 'y', 'cyan', 'magenta', 'Chocolate', 'Crimson', 'DeepSkyBlue', 'Khaki']):\n",
        "        self.ensemble_curve_model = ensemble_curve_model\n",
        "        self.colors = colors\n",
        "        assert len(colors) >= len(ensemble_curve_model.all_models), \"Not enough colors for plot all fit models. Supply more colors!\"\n",
        "        #TODO: always use the same color for the same model!\n",
        "        self.model_colors = {model.name: model_color for model, model_color in zip(ensemble_curve_model.all_models, colors)}\n",
        "\n",
        "    def posterior_plot(self, x, y, vline=None, num_curves=100, x_label=\"epochs\", y_label=\"accuracy\", x_axis_scale=0.1):\n",
        "        \"\"\"\n",
        "            x, y: data the posterior will be plotted upon.\n",
        "            num_curves: the number of curves to plot\n",
        "            x_axis_scale: scale the values on the xaxis (only for plotting but not passed to the function)\n",
        "        \"\"\"\n",
        "\n",
        "        x_plot = x_axis_scale*np.asarray(x)\n",
        "\n",
        "        for i in range(num_curves):\n",
        "            predictive_mu, sigma, color = self.get_random_curve(x)\n",
        "\n",
        "            plot(x_plot, predictive_mu, color=color, alpha=0.1)\n",
        "\n",
        "            fill_between(x_plot, predictive_mu-2*sigma, predictive_mu+2*sigma, color=color, alpha=0.01)\n",
        "\n",
        "\n",
        "        plot(x_plot, y, color=\"k\", lw=2, alpha=0.8)\n",
        "        if vline is not None:\n",
        "            axvline(x_axis_scale*vline, color=\"k\")\n",
        "        ylim(0, 1)\n",
        "        xlabel(x_label)\n",
        "        ylabel(y_label)\n",
        "        tight_layout()\n",
        "\n",
        "    def get_random_curve(self, x):\n",
        "        raise NotImplementedError(\"get_random_curve needs to be overriden\")\n",
        "\n",
        "\n",
        "class CurveModelEnsemblePlotter(EnsemblePlotter):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CurveModelEnsemblePlotter, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def get_random_curve(self, x):\n",
        "        \"\"\"\n",
        "            Sample a single curve under the given ensemble model.\n",
        "        \"\"\"\n",
        "        #sample model:\n",
        "        model_idx = np.random.multinomial(1, self.ensemble_curve_model.model_probabilities).argmax()\n",
        "        model = self.ensemble_curve_model.fit_models[model_idx]\n",
        "        if model.name in self.model_colors:\n",
        "            model_color = self.model_colors[model.name]\n",
        "        else:\n",
        "            print(\"No color defined for model %s\" % model.name)\n",
        "        #sample curve:\n",
        "        model_samples = model.get_burned_in_samples()   \n",
        "        theta_idx = np.random.randint(0, model_samples.shape[0], 1)[0]\n",
        "        theta = model_samples[theta_idx, :]\n",
        "\n",
        "        params, sigma = model.split_theta(theta)\n",
        "        predictive_mu = model.function(x, **params)\n",
        "        return predictive_mu, sigma, model_color\n",
        "\n",
        "\n",
        "class CurveEnsemblePlotter(EnsemblePlotter):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CurveEnsemblePlotter, self).__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "    def get_random_curve(self, x):\n",
        "        #sample model:\n",
        "        model_idx = np.random.multinomial(1, self.ensemble_curve_model.model_probabilities).argmax()\n",
        "        model = self.ensemble_curve_model.fit_models[model_idx]\n",
        "        model_theta_probabilities = self.ensemble_curve_model.model_theta_probabilities[model_idx]\n",
        "\n",
        "        if model.name in self.model_colors:\n",
        "            model_color = self.model_colors[model.name]\n",
        "        else:\n",
        "            print(\"No color defined for model %s\" % model.name)\n",
        "\n",
        "        model_samples = model.get_burned_in_samples()\n",
        "\n",
        "        theta_idx = np.random.multinomial(1, model_theta_probabilities).argmax()\n",
        "        theta = model_samples[theta_idx, :]\n",
        "        params, sigma = model.split_theta(theta)\n",
        "        predictive_mu = model.function(x, **params)\n",
        "        return predictive_mu, sigma, model_color"
      ],
      "metadata": {
        "id": "dF8Lj5FgmgKa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Factory**"
      ],
      "metadata": {
        "id": "Kwlr2ba1m7b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_combination(xlim,\n",
        "        models=curve_combination_models,\n",
        "\t\trecency_weighting=False,\n",
        "\t\tnormalize_weights=True,\n",
        "\t\tmonotonicity_constraint=False,\n",
        "\t\tsoft_monotonicity_constraint=True,\n",
        "\t\tnthreads=1):\n",
        "\n",
        "    curve_models = []\n",
        "    for model_name in models:\n",
        "        if model_name == \"linear\":\n",
        "            m = LinearCurveModel()\n",
        "        else:\n",
        "            if model_name in model_defaults:\n",
        "                m = MLCurveModel(function=all_models[model_name],\n",
        "                  default_vals=model_defaults[model_name],\n",
        "                  recency_weighting=recency_weighting)\n",
        "            else:\n",
        "                m = MLCurveModel(function=all_models[model_name],\n",
        "                  recency_weighting=recency_weighting)\n",
        "        curve_models.append(m)\n",
        "\n",
        "    model_combination = MCMCCurveModelCombination(curve_models,\n",
        "        xlim=xlim,\n",
        "        recency_weighting=recency_weighting,\n",
        "        normalize_weights=normalize_weights,\n",
        "        monotonicity_constraint=monotonicity_constraint,\n",
        "        soft_monotonicity_constraint=soft_monotonicity_constraint,\n",
        "        nthreads=nthreads)\n",
        "    return model_combination\n",
        "\n",
        "\n",
        "pool = None\n",
        "def setup_curve_model(ensemble_type=\"curve_model\", recency_weighting=False, pool_size=16):\n",
        "    \"\"\"\n",
        "        type: either curve_model or curve\n",
        "    \"\"\"\n",
        "    if pool_size > 1:\n",
        "        pool = Pool(pool_size)\n",
        "        map_fun = pool.map\n",
        "    else:\n",
        "        map_fun = map\n",
        "\n",
        "    ensemble_models = []\n",
        "    for model_name in curve_ensemble_models:\n",
        "        if model_name == \"linear\":\n",
        "            m = LinearCurveModel()\n",
        "        else:\n",
        "            if model_name in model_defaults:\n",
        "                m = MLCurveModel(function=all_models[model_name],\n",
        "                  default_vals=model_defaults[model_name],\n",
        "                  recency_weighting=recency_weighting)\n",
        "            else:\n",
        "                m = MLCurveModel(function=all_models[model_name],\n",
        "                  recency_weighting=recency_weighting)\n",
        "        ensemble_models.append(m)\n",
        "\n",
        "    if ensemble_type == \"curve_model\":\n",
        "        ensemble_curve_model = CurveModelEnsemble(ensemble_models, map=map_fun)\n",
        "    elif ensemble_type == \"curve\":\n",
        "        ensemble_curve_model = CurveEnsemble(ensemble_models, map=map_fun)\n",
        "    else:\n",
        "        assert False, \"unkown ensemble type\"\n",
        "    \n",
        "    return ensemble_curve_model\n",
        "\n",
        "\n",
        "def create_model(model_type, xlim, nthreads=1, recency_weighting=False):\n",
        "\t\"\"\"\n",
        "\t\ttype: either curve_combination, curve_model or curve\n",
        "\t\t\t\tcurve_combination: Bayesian Model curve_combination\n",
        "\t\t\t\tcurve_model: Bayesian Model Averaging\n",
        "\t\txlim: the target point that we want to predict eventually\n",
        "\t\tnthreads: 1 for no parallelization\n",
        "\t\"\"\"\n",
        "\tif model_type == \"curve_combination\":\n",
        "\t\tcurve_model = setup_model_combination(\n",
        "            xlim=xlim,\n",
        "            recency_weighting=recency_weighting,\n",
        "            nthreads=nthreads)\n",
        "\telif model_type == \"curve_model\" or model_type == \"curve\":\n",
        "\t\tcurve_model = setup_curve_model(\n",
        "            ensemble_type=model_type,\n",
        "            recency_weighting=recency_weighting,\n",
        "            pool_size=nthreads)\n",
        "\treturn curve_model\n",
        "\n",
        "\n",
        "def create_plotter(model_type, model):\n",
        "\tif model_type == \"curve_combination\":\n",
        "\t\treturn MCMCCurveModelCombinationPlotter(model)\n",
        "\telif model_type == \"curve_model\" or model_type == \"curve\":\n",
        "\t\treturn CurveModelEnsemblePlotter(ensemble_curve_model)"
      ],
      "metadata": {
        "id": "Yi-hCS5hnEnf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Termination Criterion**"
      ],
      "metadata": {
        "id": "ax4P9ZlenQh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMPROVEMENT_PROB_THRESHOLD = 0.05\n",
        "PREDICTIVE_STD_THRESHOLD = 0.005\n",
        "\n",
        "PREDICTION_THINNING = 10\n",
        "NTHREADS = 4\n",
        "\n",
        "\n",
        "def cut_beginning(y, threshold=0.05, look_ahead=5): #Identical to author's code\n",
        "\n",
        "  # Number of steps so far should be greater than look_ahead\n",
        "  if len(y) < look_ahead:\n",
        "    return y\n",
        "\n",
        "  num_cut = 0\n",
        "\n",
        "  for idx in range(len(y)-look_ahead):\n",
        "    start_here = True\n",
        "    for idx_ahead in range(idx, idx+look_ahead):\n",
        "      if not (y[idx_ahead] - y[0] > threshold):\n",
        "        start_here = False\n",
        "    if start_here:\n",
        "      num_cut = idx\n",
        "      break\n",
        "  return y[num_cut:]\n",
        "\n",
        "#Reimplemented:\n",
        "def get_xlim(max_iter, test_interval):\n",
        "  # Computes the number of iterative steps at which we want to predict the accuracy\n",
        "  return max_iter / test_interval\n",
        "\n",
        "# Mostly changed\n",
        "class TerminationCriterion(object):\n",
        "  def __init__(self, nthreads, prob_x_greater_type, max_iter, test_interval, predictive_std_threshold=PREDICTIVE_STD_THRESHOLD):\n",
        "    self.prob_x_greater_type = prob_x_greater_type\n",
        "    print(\"prob_x_greater_type: %s\" % prob_x_greater_type)\n",
        "    models = [\"vap\", \"ilog2\", \"weibull\", \"pow3\", \"pow4\", \"loglog_linear\",\n",
        "              \"mmf\", \"janoschek\", \"dr_hill_zero_background\", \"log_power\",\n",
        "              \"exp4\"]\n",
        "    xlim = get_xlim(max_iter, test_interval)\n",
        "    print(\"xlim: %d\" % (xlim))\n",
        "    self.xlim = xlim\n",
        "    model = setup_model_combination(models=models,\n",
        "                                    xlim=xlim,\n",
        "                                    recency_weighting=True,\n",
        "                                    nthreads=nthreads)\n",
        "    self.model = model\n",
        "    self.predictive_std_threshold = predictive_std_threshold\n",
        "\n",
        "  def run(self, ybest, y):\n",
        "    y_crt_best = np.max(y)\n",
        "    y = cut_beginning(y)\n",
        "    x = np.asarray(range(1, len(y)+1))\n",
        "\n",
        "    if not self.model.fit(x, y):\n",
        "      #failed fitting... not cancelling\n",
        "      return 0\n",
        "\n",
        "    predictive_std = self.model.predictive_std(self.xlim, thin=PREDICTION_THINNING)\n",
        "    print(\"predictive_std: %f\" % predictive_std)\n",
        "\n",
        "    if predictive_std < self.predictive_std_threshold:\n",
        "      #the model is pretty sure about the prediction: stop!\n",
        "      print(\"predictive_std low, predicting and stopping...\")\n",
        "      return self.predict()\n",
        "    elif ybest is not None:\n",
        "      print(\"predictive_std high, let's check the probably to get higher than the current ybest\")\n",
        "      #we're still checking if maybe all the probability is below ybest\n",
        "      if self.prob_x_greater_type == \"posterior_prob_x_greater_than\":\n",
        "        prob_gt_ybest_xlast = self.model.posterior_prob_x_greater_than(self.xlim,\n",
        "          ybest, thin=PREDICTION_THINNING)\n",
        "      else:\n",
        "        prob_gt_ybest_xlast = self.model.posterior_mean_prob_x_greater_than(self.xlim,\n",
        "          ybest, thin=PREDICTION_THINNING)\n",
        "\n",
        "      print(\"p(y>y_best) = %f\" % prob_gt_ybest_xlast)\n",
        "\n",
        "      if prob_gt_ybest_xlast < IMPROVEMENT_PROB_THRESHOLD:\n",
        "        return self.predict()\n",
        "      else:\n",
        "        print(\"continue evaluating\")\n",
        "        #we are probably still going to improve\n",
        "        return 0\n",
        "    else:\n",
        "      print(\"neither the predictive_std is low nor is there a ybest ... continue\")\n",
        "      return 0\n",
        "\n",
        "  def predict(self):\n",
        "    y_predict = self.model.predict(self.xlim, thin=PREDICTION_THINNING)\n",
        "    print(\"probably only going to reach %f, stopping...\" % y_predict)\n",
        "    return y_predict"
      ],
      "metadata": {
        "id": "l5hYX1OjngxG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "AzzQXbG-nj2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def logpdf(y, loc=0, scale=1):\n",
        "  y = (y - loc) / scale\n",
        "  return math.exp(-y**2/2)/(math.sqrt(2)*math.pi)\n",
        "\n",
        "def cdf(y, loc=0, scale=1):\n",
        "  y = (y - loc) / scale\n",
        "  l = 1 / np.mean(y)\n",
        "  return 1 - math.exp(-l * y)\n",
        "\n",
        "datasets = [3, 12, 23, 31, 54, 181, 188, 1457, 1461, 1464, 1468, 1475, 1485, 1486,\n",
        "            1487, 1489, 1494, 1515, 1590, 4134, 4135, 4534, 4538, 4541, 23512,\n",
        "            23517, 40498, 40668, 40670, 40685, 40701, 40900, 40975, 40978, 40981,\n",
        "            40982, 40983, 40984, 40996, 41027, 41138, 41142, 41143, 41144, 41145,\n",
        "            41146, 41147, 41156, 41157, 41158, 41159, 41161, 41162, 41163, 41164,\n",
        "            41165, 41166, 41168, 41169, 42732, 42733, 42734]\n",
        "\n",
        "\n",
        "ds = openml.datasets.get_dataset(12)\n",
        "df = ds.get_data()[0].dropna()\n",
        "y = np.array(df[ds.default_target_attribute].values)\n",
        "X = np.array(df.drop(columns=[ds.default_target_attribute], axis=1))\n",
        "\n",
        "learners = [LinearSVC(), DecisionTreeClassifier(), ExtraTreeClassifier(), ExtraTreesClassifier(), RandomForestClassifier(),\n",
        "            GradientBoostingClassifier(), LogisticRegression(), PassiveAggressiveClassifier(), Perceptron(), RidgeClassifier(),\n",
        "            SGDClassifier(), MLPClassifier(), LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), BernoulliNB(),\n",
        "            MultinomialNB(), KNeighborsClassifier()]\n",
        "\n",
        "def scrambled(orig):\n",
        "    dest = orig[:]\n",
        "    random.seed(1)\n",
        "    random.shuffle(dest)\n",
        "    return dest\n",
        "\n",
        "def results(dataset_id, learners=learners, folds=10):\n",
        "  results = {}\n",
        "\n",
        "  ds = openml.datasets.get_dataset(dataset_id)\n",
        "  print(ds)\n",
        "  df = ds.get_data()[0].dropna()\n",
        "  y = np.array(df[ds.default_target_attribute].values)\n",
        "  X = np.array(df.drop(columns=[ds.default_target_attribute], axis=1))\n",
        "  X = scrambled(X)\n",
        "  y = scrambled(y)\n",
        "  X_train, y_train, X_test, y_test = train_test_split(X, y, 0.8)\n",
        "\n",
        "  # See how long it takes to perform HPO without PTC\n",
        "  random.seed(dataset_id)\n",
        "  best_acc = 0\n",
        "  l_best = None\n",
        "  start_time = time.time()\n",
        "  for i in range(30):\n",
        "    print(\"ITERATION\", i)\n",
        "    l = clone(random.choice(learners))\n",
        "    print(\"CHOSEN LEARNER:\", l)\n",
        "    accs = []\n",
        "    l.fit(X_train, y_train)\n",
        "    y_pred = l.predict(X_test)\n",
        "    acc = accuracy_score(y_pred, y_test)\n",
        "    if acc > best_acc:\n",
        "      best_acc = acc\n",
        "      print(\"NEW BEST ACCURACY: \", best_acc, \"FOR MODEL\", l)\n",
        "      l_best = l\n",
        "  train_time = time.time() - start_time\n",
        "    \n",
        "  results[\"no PTC learner\"] = l_best\n",
        "  results[\"no PTC accuracy\"] = best_acc\n",
        "  results[\"no PTC time\"] = train_time\n",
        "\n",
        "  # See how long it takes to perform HPO with PTC\n",
        "  random.seed(dataset_id)\n",
        "  best_acc = 0\n",
        "  X_train = list(split(X_train, 10))\n",
        "  y_train = list(split(y_train, 10))\n",
        "  test_interval = 1\n",
        "  folds = 10\n",
        "  prob_x_greater_type = \"posterior_prob_x_greater_than\"\n",
        "  nthreads = NTHREADS\n",
        "  predictive_std_threshold = PREDICTIVE_STD_THRESHOLD\n",
        "  l_best = None\n",
        "\n",
        "  start_time = time.time()\n",
        "  for i in range(30):\n",
        "    print(\"ITERATION\", i)\n",
        "    l = clone(random.choice(learners))\n",
        "    print(\"CHOSEN LEARNER:\", l)\n",
        "    term_crit = TerminationCriterion(nthreads,\n",
        "                  prob_x_greater_type, folds, test_interval,\n",
        "                  predictive_std_threshold=predictive_std_threshold)\n",
        "    accs = []\n",
        "    for f in range(folds):\n",
        "      l.fit(X_train[f], y_train[f])\n",
        "      y_pred = l.predict(X_test)\n",
        "      accs.append(accuracy_score(y_pred, y_test))\n",
        "      acc_pred = term_crit.run(best_acc, accs)\n",
        "      if acc_pred < 0 or acc_pred > 1:\n",
        "          continue\n",
        "      print(\"Y POINTS:\", accs)\n",
        "      if not(acc_pred == 0.0):\n",
        "        break\n",
        "    if acc_pred==0.0:\n",
        "      acc_pred = accs[-1]\n",
        "    print(\"ACCURACY PREDICTED\", acc_pred, \"FOR MODEL\", l)\n",
        "    if acc_pred > best_acc:\n",
        "      best_acc = acc_pred\n",
        "      print(\"NEW BEST ACCURACY: \", best_acc, \"FOR MODEL\", l)\n",
        "      l_best = l\n",
        "  train_time = time.time() - start_time\n",
        "    \n",
        "  results[\"PTC learner\"] = l_best\n",
        "  results[\"PTC accuracy\"] = best_acc\n",
        "  results[\"PTC time\"] = train_time\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "for dataset_id in [12,54,1464,1468,23517,40670,40900]:\n",
        "    dic = results(dataset_id, learners, 10)\n",
        "    print(dic)\n",
        "    f = open('_results_'+str(dataset_id)+'.txt', 'w')\n",
        "    f.write(str(dic))\n",
        "    f.close() "
      ],
      "metadata": {
        "id": "TLFT9A1pnmd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}